{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Group Name : Unfiltered Commentary\n",
    "\n",
    "- Raees Moosa : 2322203\n",
    "- Oriinga Maudu : 2433303\n",
    "- Tumi Jourdan : 2180153\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is just for me since gpu bugged'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"this is just for me since gpu bugged\"\"\"\n",
    "# %env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from skimage import transform as sktf\n",
    "from skimage.util import random_noise\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html - documetnation on how to make a pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing \n",
    "- dataloaders\n",
    "- augmentation pipeline\n",
    "## Add notes on this here (what is happening)\n",
    "- added some transforms that are vert and hor flips including rotation\n",
    "- [ ] TODO Add random noise shapes in the image outside the mask maybe that will help the model learn better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "# the images are loaded as float32 and normalised\n",
    "# the mask is thresholded at 0.5 \n",
    "\"\"\" the permute is needed since the format for image tensors must be (C, H, W)\n",
    "But when we read from opencv the shape is (H, W, C)\n",
    "and the mask must be of dim (1, H, W) since single channel - unsqueeze add this channel\n",
    "\"\"\"\n",
    "# returned as tensors \n",
    "\n",
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, num_transforms=0,include_inverse_mask=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.num_transforms = num_transforms\n",
    "        self.include_inverse_mask=include_inverse_mask\n",
    "        images = sorted(os.listdir(img_dir))\n",
    "        masks = sorted(os.listdir(mask_dir))\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            img_path = os.path.join(self.img_dir, images[i])\n",
    "            mask_path = os.path.join(self.mask_dir, masks[i])\n",
    "\n",
    "          \n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "            image = image.astype(np.float32)/255.0\n",
    "            # image = Image.fromarray(image)  \n",
    "\n",
    "            \n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (512,512))\n",
    "            mask = (mask > 0.5).astype(np.float32) \n",
    "            # mask = Image.fromarray(mask)  #   PIL image needed for transforms\n",
    "\n",
    "            # store the original image and mask\n",
    "            self.append_image_mask(image, mask)\n",
    "\n",
    "            # do transformations \n",
    "            for _ in range(self.num_transforms):\n",
    "                transformed_image, transformed_mask = self.apply_transform(image, mask)\n",
    "                self.append_image_mask(transformed_image, transformed_mask)\n",
    "\n",
    "    def apply_transform(self, image, mask):\n",
    "        \"\"\"Apply deterministic transformations to both image and mask\n",
    "        This is imortant since using the torchvision.transforms was givin a random transform\n",
    "        for both image and mask -> they didn't match up\"\"\"\n",
    "        if self.transform:\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                image = np.fliplr(image)\n",
    "                mask = np.fliplr(mask)\n",
    "\n",
    "           \n",
    "            if random.random() > 0.5:\n",
    "                image = np.flipud(image)\n",
    "                mask = np.flipud(mask)\n",
    "\n",
    "            # Apply rotation deterministically\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            image = sktf.rotate(image, angle, mode=\"edge\" , preserve_range=True)\n",
    "            mask = sktf.rotate(mask, angle, mode=\"edge\" , preserve_range=True)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def append_image_mask(self, image, mask):\n",
    "        \"\"\"need to store them as tensors.\"\"\"\n",
    "        image = torch.tensor(image.transpose((2, 0, 1)), dtype=torch.float32) # (C, H, W)\n",
    "        mask = torch.tensor(mask[None, ...], dtype=torch.float32)   # (1, H, W)\n",
    "\n",
    "       \n",
    "        if(self.include_inverse_mask):\n",
    "            inverse_mask = 1 - mask\n",
    "            combined_mask = torch.cat([inverse_mask, mask], dim=0)  # Combined (2, H, W)\n",
    "\n",
    "        \n",
    "            self.data.append((image, combined_mask))\n",
    "        else:\n",
    "            self.data.append((image,mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset\n",
    "- NOTE increasing batch size to 5 since our dataset size is up to 40 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "torch.Size([2, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Axes: >, <matplotlib.image.AxesImage at 0x239a9c505d0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  list of transformations\n",
    "# transform_list = [\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.RandomVerticalFlip(),\n",
    "#     T.RandomRotation(30),\n",
    "# ]\n",
    "\n",
    "\n",
    "train_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/train/\",\n",
    "    mask_dir=\"./masks-1024x768/train/\", \n",
    "    transform=True,\n",
    "    num_transforms=3  \n",
    ")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "# note we now have more images since transform applied so maybe adjust batches\n",
    "train_loader = DataLoader(train_dataset,batch_size =2, shuffle=True)\n",
    "\n",
    "print(len(train_dataset.data))\n",
    "# # to visualise the images + masks are in correct pairing\n",
    "image, mask = train_dataset[6]\n",
    "\n",
    "image_np = (image.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "mask_np = mask.permute(1, 2, 0).numpy()\n",
    "print(mask.shape)\n",
    "\n",
    "\n",
    "#to collapse the separate channels of bg and fg mask\n",
    "if mask_np.shape[2] == 2:\n",
    "    mask_np = np.argmax(mask_np, axis=2)\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(1,2,1), plt.imshow(image_np)\n",
    "plt.subplot(1,2,2), plt.imshow(mask_np, cmap=\"gray\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/val/\",\n",
    "    mask_dir=\"./masks-1024x768/val/\", \n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_dataset = PuzzleDataset(img_dir = \"./images-1024x768/test/\",\n",
    "                            mask_dir = \"./masks-1024x768/test/\")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "test_loader = DataLoader(test_dataset,batch_size =1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Unet Construction\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_epoch = load_checkpoint(model, optimizer)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Adding checkpoints as required and to avoid training everytime someone wants to test\"\"\"\n",
    "def save_checkpoint(model, optimizer, epoch, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}.\")\n",
    "\n",
    "\n",
    "# use as in training\n",
    "\"\"\"save_checkpoint(model, optimizer, epoch)\"\"\"\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded from epoch {epoch}.\")\n",
    "        return epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found.\")\n",
    "        return 0  # training from start ...\n",
    "\n",
    "# before training starts to load model\n",
    "\"\"\"start_epoch = load_checkpoint(model, optimizer)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" helper to clear the gpu of datasets and model\"\"\"\n",
    "def clear_gpu_memory(model=None, data_loaders=None):\n",
    "    \n",
    "    if model is not None:\n",
    "        model.cpu()\n",
    "        del model\n",
    "    \n",
    "    \n",
    "    if data_loaders is not None:\n",
    "        for loader in data_loaders:\n",
    "            del loader  \n",
    "    \n",
    "    #  garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 1 : Using `torch.nn.ConvTranspose2d` for upsampling\n",
    "- We removed the softmax in the unet with convolve to get to the required number of output classes (ask richard).\n",
    "- Having the argmax inside the network caused issues with backprop so we only compute the raw logits and use these for the BCE loss\n",
    "then use the apply softmax outside when performing inference on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this is the triple convolution, chat gpt says we should use normalization dont know if we should keep it\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.triple_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.triple_conv(x)\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv, self).__init__()\n",
    "        self.triple_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.triple_conv(x)\n",
    "\n",
    "    \n",
    "# the down module is what the unet uses during the first half \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "            DoubleConv(in_channels,out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret_ = self.conv_pool(x)\n",
    "        return ret_\n",
    "\n",
    "# up transpose, \n",
    "class UpConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConvTranspose, self).__init__()\n",
    "        # to determine amount of out channels\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = Conv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpBilinear(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpBilinear, self).__init__()\n",
    "        # the bilinear is provided by the nn module, we set the mode to bilinear here\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = Conv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return self.conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,mode):\n",
    "        super(Up, self).__init__()\n",
    "        if(mode=='convtranspose'):\n",
    "            self.conv_pool = nn.Sequential(\n",
    "                UpConvTranspose(in_channels,out_channels),\n",
    "                Conv(out_channels,out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv_pool = nn.Sequential(\n",
    "                UpBilinear(in_channels,out_channels),\n",
    "                Conv(out_channels,out_channels)\n",
    "            )\n",
    "        \n",
    "        self.dconv = DoubleConv(out_channels*2,out_channels)\n",
    "    \n",
    "    def forward(self,x1,x2):\n",
    "        x = self.conv_pool(x1)\n",
    "        return self.dconv(torch.cat([x,x2],dim=1))\n",
    "\n",
    "    \n",
    "class SoftMax(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return torch.argmax(x,dim=1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, variant='convtranspose'):\n",
    "        super(Unet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.variant = variant\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64,128)\n",
    "        self.down2 = Down(128,256)\n",
    "        self.down3 = Down(256,512)\n",
    "        self.down4 = Down(512,1024)\n",
    "        \n",
    "        self.up1 = Up(1024,512,variant)\n",
    "        self.up2 = Up(512, 256,variant)\n",
    "        self.up3 = Up(256, 128,variant)\n",
    "        self.up4 = Up(128, 64,variant)\n",
    "\n",
    "        self.outc = nn.Conv2d(64,2, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(preds, targets, threshold=0.5):\n",
    "    # Make sure predictions and targets are 4D: [batch_size, channels, height, width]\n",
    "    if preds.dim() == 3:\n",
    "        preds = preds.unsqueeze(0)  # Add batch dimension\n",
    "    if targets.dim() == 3:\n",
    "        targets = targets.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    preds = (torch.sigmoid(preds) > threshold).float()\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = torch.sum(preds * targets, dim=[2, 3])\n",
    "    union = torch.sum(preds, dim=[2, 3]) + torch.sum(targets, dim=[2, 3]) - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)  # Add small epsilon to avoid division by zero\n",
    "    \n",
    "    return iou.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I just pushed to the cpu when I try putting the model on the gpu I get weird errors in training \n",
    "that I used up all the GPU memory maybe you wont get this error then just comment out `device=\"cpu\"\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnet variant 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# I forced the device to cpu since I have no gpu comment that line out!\u001b[39;00m\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "\"\"\" Training UNET Variant 1\n",
    "    torch.nn.ConvTranspose2d \"\"\"\n",
    "\n",
    "import gc\n",
    "import wandb\n",
    "wandb.init(project=\"Unet variant 1\")\n",
    "\n",
    "# I forced the device to cpu since I have no gpu comment that line out!\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Model, criterion, and optimizer setup\n",
    "model = Unet(n_channels=3, n_classes=2, variant='convtranspose')\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = model.to('cuda:1')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "#  checkpoints if any\n",
    "start_epoch = load_checkpoint(model, optimizer, filename=\"var1_checkpoint.pth\")\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    \n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "    \n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    train_iou = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # outputs = outputs.float()\n",
    "        # outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        # print(f\"Outputs min: {outputs.min()}, max: {outputs.max()}, shape: {outputs.shape}\")\n",
    "        # print(f\"Masks min: {masks.min()}, max: {masks.max()}, shape: {masks.shape}\")\n",
    "        \n",
    "        # BCE loss with logits \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        #gradient clipping I dunno why we get nans\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        #softmax to logits to then get the IoU\n",
    "        \n",
    "        \n",
    "        #  loss and IoU\n",
    "        running_loss += loss.item()\n",
    "        train_iou += calculate_iou(outputs, masks)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_iou = train_iou / len(train_loader)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            val_iou += calculate_iou(outputs, masks)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_iou = val_iou/ len(val_loader)\n",
    "\n",
    "\n",
    "    #  training and validation metrics to wandb\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"train_iou\": train_iou, \"val_iou\": val_iou})\n",
    "    \n",
    "    \n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "    \n",
    "    # store model checkpoint if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint(model,optimizer, epoch, \"var1_checkpoint.pth\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    # model.cpu()\n",
    "    # del model\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "\n",
    "# close wandb run\n",
    "\n",
    "\n",
    "# run tensorboard --logdir=runs to see networkls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor_as_image(tensor, channel_num, channel_index, height_index, width_index):\n",
    "    # Move the tensor to CPU and convert it to a NumPy array\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    if channel_index == 1:\n",
    "        tensor_np = tensor_np.squeeze(0)\n",
    "\n",
    "        channel_index -=1\n",
    "        height_index-=1\n",
    "        width_index-=1\n",
    "        \n",
    "    # Handle single-channel (grayscale) image\n",
    "    if channel_num == 1:\n",
    "        image_np = tensor_np.squeeze(channel_index)  # Remove the channel dimension\n",
    "        plt.imshow(image_np, cmap=\"gray\")\n",
    "        plt.title(\"Single-channel image\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Handle two-channel image (display channels separately)\n",
    "    elif channel_num == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create 1 row, 2 columns\n",
    "        for i in range(2):\n",
    "            channel_image = tensor_np[i]  # Select each channel (e.g., 0 and 1)\n",
    "            axes[i].imshow(channel_image, cmap=\"gray\")\n",
    "            axes[i].set_title(f\"Channel {i}\")\n",
    "            # print(f\"Max value in channel {i}:\", np.max(channel_image))\n",
    "            # print(f\"Min value in channel {i}:\", np.min(channel_image))\n",
    "        plt.show()\n",
    "    \n",
    "    # Handle three-channel image (RGB)\n",
    "    elif channel_num == 3:\n",
    "        print(tensor_np.shape)\n",
    "        # Transpose from (channels, height, width) to (height, width, channels)\n",
    "        image_np = np.transpose(tensor_np, (height_index, width_index, channel_index))\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(\"Three-channel image (RGB)\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_loader(model, test_loader, device, show_plot=False):\n",
    "    model.eval() \n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, mask in test_loader:\n",
    "            images = image.to(device)\n",
    "            masks = mask.to(device)  \n",
    "            \n",
    "            outputs = model(images)\n",
    "            total_iou += calculate_iou(predicted_mask, masks) \n",
    "            #  softmax\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            #  predicted mask\n",
    "            predicted_mask = torch.argmax(probs, dim=1)  #dim=1 for batch predictions\n",
    "\n",
    "            # IoU for the current batch\n",
    "            \n",
    "            num_samples += 1\n",
    "\n",
    "            if show_plot:\n",
    "                display_tensor_as_image(images, 3, 1, 2, 3)\n",
    "                image_np = predicted_mask.squeeze().cpu().numpy()\n",
    "                plt.imshow(image_np, vmin=0, vmax=1, cmap='gray')\n",
    "                plt.axis('off')  \n",
    "                plt.show()\n",
    "\n",
    "    mean_iou = total_iou / len(test_loader)\n",
    "    print(f\"Mean IoU on the test set: {mean_iou:.4f}\")\n",
    "    \n",
    "    # mean IoU to wandb\n",
    "    # wandb.log({\"test_mean_iou\": mean_iou})\n",
    "    \n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet(n_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvtranspose\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "del model\n",
    "model = Unet(n_channels=3, n_classes=2, variant='convtranspose')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "load_checkpoint(model, optimizer, filename=\"var1_checkpoint.pth\")\n",
    "\n",
    "mean_iou = evaluate_model_on_test_loader(model, test_loader, device, show_plot=True)\n",
    "wandb.log({\"test_mean_iou\": mean_iou})\n",
    "wandb.finish()\n",
    "\"\"\" DELETING THE MODEL HERE TO FREE UP VRAM!!!!\n",
    " BUT ALSO so i dont have rename the model variable from the  previous block in later sections\"\"\"\n",
    "clear_gpu_memory(model, [train_loader, val_loader, test_loader])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 2: Using `torch.nn.Upsample` for bilinear upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home-mscluster/remoosa/ComputerVisionLab/Lab3/wandb/run-20240920_225746-6i4hh26y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2322203-witwatersrand-university/Unet%20variant%202%20-%20bilinear%20upsampling/runs/6i4hh26y' target=\"_blank\">dark-resonance-4</a></strong> to <a href='https://wandb.ai/2322203-witwatersrand-university/Unet%20variant%202%20-%20bilinear%20upsampling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2322203-witwatersrand-university/Unet%20variant%202%20-%20bilinear%20upsampling' target=\"_blank\">https://wandb.ai/2322203-witwatersrand-university/Unet%20variant%202%20-%20bilinear%20upsampling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2322203-witwatersrand-university/Unet%20variant%202%20-%20bilinear%20upsampling/runs/6i4hh26y' target=\"_blank\">https://wandb.ai/2322203-witwatersrand-university/Unet%20variant%202%20-%20bilinear%20upsampling/runs/6i4hh26y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found.\n",
      "Checkpoint saved at epoch 1\n",
      "Checkpoint saved at epoch 2\n",
      "Checkpoint saved at epoch 3\n",
      "Checkpoint saved at epoch 4\n",
      "Checkpoint saved at epoch 5\n",
      "Checkpoint saved at epoch 9\n",
      "Checkpoint saved at epoch 12\n",
      "Checkpoint saved at epoch 13\n",
      "Checkpoint saved at epoch 16\n",
      "Checkpoint saved at epoch 33\n",
      "Checkpoint saved at epoch 34\n",
      "Checkpoint saved at epoch 45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"Unet variant 2 - bilinear upsampling\")\n",
    "\n",
    "\n",
    "model = Unet(n_channels=3, n_classes=2,variant='upsampling')\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = model.to('cuda:1')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "#  checkpoints if any\n",
    "start_epoch = load_checkpoint(model, optimizer, filename=\"var2_checkpoint.pth\")\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    \n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "    \n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    train_iou = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # outputs = outputs.float()\n",
    "        # outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        # print(f\"Outputs min: {outputs.min()}, max: {outputs.max()}, shape: {outputs.shape}\")\n",
    "        # print(f\"Masks min: {masks.min()}, max: {masks.max()}, shape: {masks.shape}\")\n",
    "        \n",
    "        # BCE loss with logits \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        #gradient clipping I dunno why we get nans\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        #  loss and IoU\n",
    "        running_loss += loss.item()\n",
    "        train_iou += calculate_iou(outputs, masks)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_iou = train_iou/ len(train_loader)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            val_iou += calculate_iou(outputs, masks)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_iou = val_iou /len(val_loader)\n",
    "\n",
    "\n",
    "    #  training and validation metrics to wandb\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"train_iou\": train_iou, \"val_iou\": val_iou})\n",
    "    \n",
    "    \n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "    \n",
    "    # store model checkpoint if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint(model,optimizer, epoch, \"var2_checkpoint.pth\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    # model.cpu()\n",
    "    # del model\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the second base variant here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736246/3703089074.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvar2_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m mean_iou \u001b[38;5;241m=\u001b[39m evaluate_model_on_test_loader(model, test_loader, device, show_plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_mean_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: mean_iou})\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(model, optimizer, filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename):\n\u001b[1;32m     18\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename)\n\u001b[0;32m---> 19\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "test_dataset = PuzzleDataset(img_dir = \"./images-1024x768/test/\",\n",
    "                            mask_dir = \"./masks-1024x768/test/\")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "test_loader = DataLoader(test_dataset,batch_size =1, shuffle=True)\n",
    "\n",
    "\n",
    "del model\n",
    "model = Unet(n_channels=3, n_classes=2,variant='upsampling')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "load_checkpoint(model, optimizer, filename=\"var2_checkpoint.pth\")\n",
    "\n",
    "mean_iou = evaluate_model_on_test_loader(model, test_loader, device, show_plot=True)\n",
    "wandb.log({\"test_mean_iou\": mean_iou})\n",
    "wandb.finish()\n",
    "\"\"\" DELETING THE MODEL HERE TO FREE UP VRAM!!!!\n",
    " BUT ALSO so i dont have rename the model variable from the  previous block in later sections\"\"\"\n",
    "clear_gpu_memory(model, [train_loader, val_loader, test_loader])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "evaluate model on test set \n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- F1 score\n",
    "- IoU\n",
    "\n",
    "# Select the best model based on the validation IoU and report its performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdasdasfafasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Other architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/qubvel-org/segmentation_models.pytorch\n",
    "#!pip install lightning albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/train/\",\n",
    "    mask_dir=\"./masks-1024x768/train/\", \n",
    "    transform=True,\n",
    "    num_transforms=3,\n",
    "    include_inverse_mask=False \n",
    ")\n",
    "val_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/val/\",\n",
    "    mask_dir=\"./masks-1024x768/val/\",\n",
    "    include_inverse_mask=False\n",
    ")\n",
    "\n",
    "test_dataset = PuzzleDataset(\n",
    "    img_dir = \"./images-1024x768/test/\",\n",
    "    mask_dir = \"./masks-1024x768/test/\",\n",
    "    include_inverse_mask=False\n",
    "\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=5,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=5,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "T_MAX = EPOCHS * len(train_dataloader)\n",
    "OUT_CLASSES = 1\n",
    "\n",
    "class UnetPlus(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # preprocessing parameteres for image\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "        # initialize step metics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        #print(\"batch:\",batch[0].shape)\n",
    "        image, mask = batch\n",
    "        #image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32,\n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        #mask = batch[\"mask\"]\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "        #print(f\"Logits mask shape: {logits_mask.shape}, Target mask shape: {mask.shape}\")\n",
    "\n",
    "\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then\n",
    "        # apply thresholding\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask.long(), mask.long(), mode=\"binary\"\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        losses = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(f\"val_loss\", losses, prog_bar=True)\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        #F1 score\n",
    "        precision = tp.sum() / (tp.sum() + fp.sum() + 1e-6)  # Add small value to avoid division by zero\n",
    "        recall = tp.sum() / (tp.sum() + fn.sum() + 1e-6)     # Add small value to avoid division by zero\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)    \n",
    "\n",
    "        #accuracy\n",
    "        accuracy = (tp.sum()+tn.sum())/(tp.sum(),tn.sum()+fp.sum()+fn.sum())\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image\n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(\n",
    "            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n",
    "        )\n",
    "\n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset\n",
    "        # with \"empty\" images (images without target class) a large gap could be observed.\n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "            f\"{stage}_f1_score\": f1_score, # Log the F1 score\n",
    "            f\"{stage}_precision\": precision,\n",
    "            f\"{stage}_accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\")\n",
    "        # append the metics of each step to the\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        # empty set output list\n",
    "        self.training_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\")\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\")\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        # empty set output list\n",
    "        self.test_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnet 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnetRes = UnetPlus(\"Unet\",\"resnet34\",3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(\n",
    "    UnetRes,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load model\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m UnetMobile \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./mobile/mobile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\base\\hub_mixin.py:138\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmp\u001b[39;00m\n",
      "\u001b[0;32m    137\u001b[0m model_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(smp, model_class_name)\n",
      "\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n",
      "\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hub_mixin.py:570\u001b[0m, in \u001b[0;36mModelHubMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n",
      "\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_hub_mixin_inject_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n",
      "\u001b[0;32m    568\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n",
      "\u001b[1;32m--> 570\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Implicitly set the config as instance attribute if not already set by the class\u001b[39;00m\n",
      "\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# This way `config` will be available when calling `save_pretrained` or `push_to_hub`.\u001b[39;00m\n",
      "\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hub_mixin_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, {})):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hub_mixin.py:790\u001b[0m, in \u001b[0;36mPyTorchModelHubMixin._from_pretrained\u001b[1;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n",
      "\u001b[0;32m    773\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n",
      "\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pretrained\u001b[39m(\n",
      "\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n",
      "\u001b[0;32m    788\u001b[0m ):\n",
      "\u001b[0;32m    789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 790\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(model_id):\n",
      "\u001b[0;32m    792\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights from local directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\unet\\model.py:71\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[1;34m(self, encoder_name, encoder_depth, encoder_weights, decoder_use_batchnorm, decoder_channels, decoder_attention_type, in_channels, classes, activation, aux_params)\u001b[0m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n",
      "\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m     58\u001b[0m     encoder_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     67\u001b[0m     aux_params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m     68\u001b[0m ):\n",
      "\u001b[0;32m     69\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m UnetDecoder(\n",
      "\u001b[0;32m     79\u001b[0m         encoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mout_channels,\n",
      "\u001b[0;32m     80\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39mdecoder_channels,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     84\u001b[0m         attention_type\u001b[38;5;241m=\u001b[39mdecoder_attention_type,\n",
      "\u001b[0;32m     85\u001b[0m     )\n",
      "\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m SegmentationHead(\n",
      "\u001b[0;32m     88\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39mdecoder_channels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n",
      "\u001b[0;32m     89\u001b[0m         out_channels\u001b[38;5;241m=\u001b[39mclasses,\n",
      "\u001b[0;32m     90\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivation,\n",
      "\u001b[0;32m     91\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n",
      "\u001b[0;32m     92\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\encoders\\__init__.py:86\u001b[0m, in \u001b[0;36mget_encoder\u001b[1;34m(name, in_channels, depth, weights, output_stride, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n",
      "\u001b[0;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong pretrained weights `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` for encoder `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[0;32m     83\u001b[0m                 weights, name, \u001b[38;5;28mlist\u001b[39m(encoders[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;32m     84\u001b[0m             )\n",
      "\u001b[0;32m     85\u001b[0m         )\n",
      "\u001b[1;32m---> 86\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mmodel_zoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m     88\u001b[0m encoder\u001b[38;5;241m.\u001b[39mset_in_channels(in_channels, pretrained\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m32\u001b[39m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:769\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n",
      "\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n",
      "\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n",
      "\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n",
      "\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n",
      "\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n",
      "\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n",
      "\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n",
      "\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n",
      "\u001b[1;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n",
      "\u001b[0;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n",
      "\u001b[0;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n",
      "\u001b[0;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n",
      "\u001b[0;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n",
      "\u001b[1;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[0;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n",
      "\u001b[0;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n",
      "\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n",
      "\u001b[1;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n",
      "\u001b[0;32m    389\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n",
      "\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n",
      "\u001b[1;32m--> 391\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:364\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n",
      "\u001b[0;32m    362\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    365\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    366\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    367\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    368\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;32m    370\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "UnetRes = smp.from_pretrained('./resnet/resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run validation dataset\n",
    "valid_metrics = trainer.validate(UnetRes, dataloaders=val_dataloader, verbose=False)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test dataset\n",
    "test_metrics = trainer.test(UnetRes, dataloaders=test_dataloader, verbose=False)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mean_iou = evaluate_model_on_test_loader(UnetRes, test_dataloader, device, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "smp_model = UnetRes.model\n",
    "\n",
    "commit_info = smp_model.save_pretrained(\n",
    "    save_directory=\"saved_models/UnetPlus/UnetRes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unet VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnetVGG = UnetPlus(\"Unet\",\"vgg11\",3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(\n",
    "    UnetVGG,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load model\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m UnetMobile \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./mobile/mobile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\base\\hub_mixin.py:138\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmp\u001b[39;00m\n",
      "\u001b[0;32m    137\u001b[0m model_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(smp, model_class_name)\n",
      "\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n",
      "\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hub_mixin.py:570\u001b[0m, in \u001b[0;36mModelHubMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n",
      "\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_hub_mixin_inject_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n",
      "\u001b[0;32m    568\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n",
      "\u001b[1;32m--> 570\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Implicitly set the config as instance attribute if not already set by the class\u001b[39;00m\n",
      "\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# This way `config` will be available when calling `save_pretrained` or `push_to_hub`.\u001b[39;00m\n",
      "\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hub_mixin_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, {})):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hub_mixin.py:790\u001b[0m, in \u001b[0;36mPyTorchModelHubMixin._from_pretrained\u001b[1;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n",
      "\u001b[0;32m    773\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n",
      "\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pretrained\u001b[39m(\n",
      "\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n",
      "\u001b[0;32m    788\u001b[0m ):\n",
      "\u001b[0;32m    789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 790\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(model_id):\n",
      "\u001b[0;32m    792\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights from local directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\unet\\model.py:71\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[1;34m(self, encoder_name, encoder_depth, encoder_weights, decoder_use_batchnorm, decoder_channels, decoder_attention_type, in_channels, classes, activation, aux_params)\u001b[0m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n",
      "\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m     58\u001b[0m     encoder_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     67\u001b[0m     aux_params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m     68\u001b[0m ):\n",
      "\u001b[0;32m     69\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m UnetDecoder(\n",
      "\u001b[0;32m     79\u001b[0m         encoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mout_channels,\n",
      "\u001b[0;32m     80\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39mdecoder_channels,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     84\u001b[0m         attention_type\u001b[38;5;241m=\u001b[39mdecoder_attention_type,\n",
      "\u001b[0;32m     85\u001b[0m     )\n",
      "\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m SegmentationHead(\n",
      "\u001b[0;32m     88\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39mdecoder_channels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n",
      "\u001b[0;32m     89\u001b[0m         out_channels\u001b[38;5;241m=\u001b[39mclasses,\n",
      "\u001b[0;32m     90\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivation,\n",
      "\u001b[0;32m     91\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n",
      "\u001b[0;32m     92\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\encoders\\__init__.py:86\u001b[0m, in \u001b[0;36mget_encoder\u001b[1;34m(name, in_channels, depth, weights, output_stride, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n",
      "\u001b[0;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong pretrained weights `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` for encoder `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[0;32m     83\u001b[0m                 weights, name, \u001b[38;5;28mlist\u001b[39m(encoders[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;32m     84\u001b[0m             )\n",
      "\u001b[0;32m     85\u001b[0m         )\n",
      "\u001b[1;32m---> 86\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mmodel_zoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m     88\u001b[0m encoder\u001b[38;5;241m.\u001b[39mset_in_channels(in_channels, pretrained\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m32\u001b[39m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:769\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n",
      "\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n",
      "\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n",
      "\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n",
      "\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n",
      "\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n",
      "\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n",
      "\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n",
      "\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n",
      "\u001b[1;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n",
      "\u001b[0;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n",
      "\u001b[0;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n",
      "\u001b[0;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n",
      "\u001b[0;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n",
      "\u001b[1;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[0;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n",
      "\u001b[0;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n",
      "\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n",
      "\u001b[1;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n",
      "\u001b[0;32m    389\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n",
      "\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n",
      "\u001b[1;32m--> 391\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:364\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n",
      "\u001b[0;32m    362\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    365\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    366\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    367\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    368\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;32m    370\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "UnetVGG = smp.from_pretrained('./vgg/vgg11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run validation dataset\n",
    "valid_metrics = trainer.validate(UnetVGG, dataloaders=val_dataloader, verbose=False)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test dataset\n",
    "test_metrics = trainer.test(UnetVGG, dataloaders=test_dataloader, verbose=False)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mean_iou = evaluate_model_on_test_loader(UnetVGG, test_dataloader, device, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "smp_model = UnetVGG.model\n",
    "\n",
    "commit_info = smp_model.save_pretrained(\n",
    "    save_directory=\"saved_models/UnetPlus/UnetVGG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unet Mobileone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m UnetMobile \u001b[38;5;241m=\u001b[39m \u001b[43mUnetPlus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmobileone_s2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 8\u001b[0m, in \u001b[0;36mUnetPlus.__init__\u001b[1;34m(self, arch, encoder_name, in_channels, out_classes, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arch, encoder_name, in_channels, out_classes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# preprocessing parameteres for image\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     params \u001b[38;5;241m=\u001b[39m smp\u001b[38;5;241m.\u001b[39mencoders\u001b[38;5;241m.\u001b[39mget_preprocessing_params(encoder_name)\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\__init__.py:61\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(arch, encoder_name, encoder_weights, in_channels, classes, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong architecture type `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     58\u001b[0m             arch, \u001b[38;5;28mlist\u001b[39m(archs_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     59\u001b[0m         )\n\u001b[0;32m     60\u001b[0m     )\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\unet\\model.py:71\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[1;34m(self, encoder_name, encoder_depth, encoder_weights, decoder_use_batchnorm, decoder_channels, decoder_attention_type, in_channels, classes, activation, aux_params)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     58\u001b[0m     encoder_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     aux_params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m UnetDecoder(\n\u001b[0;32m     79\u001b[0m         encoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[0;32m     80\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39mdecoder_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m         attention_type\u001b[38;5;241m=\u001b[39mdecoder_attention_type,\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m SegmentationHead(\n\u001b[0;32m     88\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39mdecoder_channels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     89\u001b[0m         out_channels\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m     90\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[0;32m     91\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     92\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\encoders\\__init__.py:86\u001b[0m, in \u001b[0;36mget_encoder\u001b[1;34m(name, in_channels, depth, weights, output_stride, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong pretrained weights `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` for encoder `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     83\u001b[0m                 weights, name, \u001b[38;5;28mlist\u001b[39m(encoders[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     85\u001b[0m         )\n\u001b[1;32m---> 86\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mmodel_zoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     88\u001b[0m encoder\u001b[38;5;241m.\u001b[39mset_in_channels(in_channels, pretrained\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m32\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:769\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 391\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:364\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    362\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    365\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    366\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    367\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    368\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    370\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "UnetMobile = UnetPlus(\"Unet\",\"mobileone_s2\",3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(\n",
    "    UnetMobile,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m UnetMobile \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./mobile/mobile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\base\\hub_mixin.py:138\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmp\u001b[39;00m\n\u001b[0;32m    137\u001b[0m model_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(smp, model_class_name)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hub_mixin.py:570\u001b[0m, in \u001b[0;36mModelHubMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_hub_mixin_inject_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m    568\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 570\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Implicitly set the config as instance attribute if not already set by the class\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# This way `config` will be available when calling `save_pretrained` or `push_to_hub`.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hub_mixin_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, {})):\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hub_mixin.py:790\u001b[0m, in \u001b[0;36mPyTorchModelHubMixin._from_pretrained\u001b[1;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pretrained\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    788\u001b[0m ):\n\u001b[0;32m    789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load Pytorch pretrained weights and return the loaded model.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(model_id):\n\u001b[0;32m    792\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights from local directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\unet\\model.py:71\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[1;34m(self, encoder_name, encoder_depth, encoder_weights, decoder_use_batchnorm, decoder_channels, decoder_attention_type, in_channels, classes, activation, aux_params)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     58\u001b[0m     encoder_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     aux_params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m UnetDecoder(\n\u001b[0;32m     79\u001b[0m         encoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[0;32m     80\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39mdecoder_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m         attention_type\u001b[38;5;241m=\u001b[39mdecoder_attention_type,\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m SegmentationHead(\n\u001b[0;32m     88\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39mdecoder_channels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     89\u001b[0m         out_channels\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m     90\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[0;32m     91\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     92\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch\\encoders\\__init__.py:86\u001b[0m, in \u001b[0;36mget_encoder\u001b[1;34m(name, in_channels, depth, weights, output_stride, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong pretrained weights `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` for encoder `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     83\u001b[0m                 weights, name, \u001b[38;5;28mlist\u001b[39m(encoders[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     85\u001b[0m         )\n\u001b[1;32m---> 86\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mmodel_zoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     88\u001b[0m encoder\u001b[38;5;241m.\u001b[39mset_in_channels(in_channels, pretrained\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m32\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:769\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 391\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Oriin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:364\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    362\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    365\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    366\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    367\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    368\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    370\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "UnetMobile = smp.from_pretrained('./mobile/mobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run validation dataset\n",
    "valid_metrics = trainer.validate(UnetMobile, dataloaders=val_dataloader, verbose=False)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test dataset\n",
    "test_metrics = trainer.test(UnetMobile, dataloaders=test_dataloader, verbose=False)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mean_iou = evaluate_model_on_test_loader(UnetVGG, test_dataloader, device, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "smp_model = UnetMobile.model\n",
    "\n",
    "commit_info = smp_model.save_pretrained(\n",
    "    save_directory=\"saved_models/UnetPlus/UnetMobile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep lab v3\n",
    "EPOCHS = 30\n",
    "T_MAX = EPOCHS * len(train_dataloader)\n",
    "OUT_CLASSES = 2\n",
    "\n",
    "class DeepV_plus(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, pretrained=\"imagenet\", **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=pretrained,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.encoder_name = encoder_name\n",
    "        self.arch_name = arch\n",
    "        self.in_channels = in_channels\n",
    "        self.out_classes = out_classes\n",
    "        # preprocessing parameteres for image\n",
    "        print(pretrained)\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name,pretrained)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "        # initialize step metics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[0]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32,\n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[1]\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then\n",
    "        # apply thresholding\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask.long(), mask.long(), mode=\"binary\"\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        \n",
    "        losses = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(f\"val_loss\", losses, prog_bar=True)\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        #F1 score\n",
    "        precision = tp.sum() / (tp.sum() + fp.sum() + 1e-6)  # Add small value to avoid division by zero\n",
    "        recall = tp.sum() / (tp.sum() + fn.sum() + 1e-6)     # Add small value to avoid division by zero\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)    \n",
    "\n",
    "        #accuracy\n",
    "        accuracy = (tp.sum()+tn.sum())/(tp.sum(),tn.sum()+fp.sum()+fn.sum())\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image\n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(\n",
    "            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n",
    "        )\n",
    "\n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset\n",
    "        # with \"empty\" images (images without target class) a large gap could be observed.\n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "            f\"{stage}_f1_score\": f1_score, # Log the F1 score\n",
    "            f\"{stage}_precision\": precision,\n",
    "            f\"{stage}_accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\")\n",
    "        # append the metics of each step to the\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        # empty set output list\n",
    "        self.training_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\")\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\")\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        # empty set output list\n",
    "        self.test_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "        return\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # Save hyperparameters in the checkpoint\n",
    "        checkpoint['hyper_parameters'] = {\n",
    "            'arch': self.arch_name,\n",
    "            'encoder_name': self.encoder_name,\n",
    "            'in_channels': self.in_channels,  # Assuming model has this attribute\n",
    "            'out_classes': self.out_classes,  # Assuming model has this attribute\n",
    "        }\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        \"\"\"\n",
    "        Load model weights from a checkpoint and instantiate the model.\n",
    "\n",
    "        Parameters:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        kwargs (dict): Additional arguments to pass to the model initialization.\n",
    "\n",
    "        Returns:\n",
    "        DeepV_plus: Model instance with weights loaded from the checkpoint.\n",
    "        \"\"\"\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "        \n",
    "        # Get model parameters from the checkpoint\n",
    "        model_params = checkpoint['hyper_parameters']\n",
    "\n",
    "        # Create a new instance of the model using parameters from the checkpoint\n",
    "        model = cls(\n",
    "            arch=kwargs.get('arch', model_params['arch']),\n",
    "            encoder_name=kwargs.get('encoder_name', model_params['encoder_name']),\n",
    "            in_channels=kwargs.get('in_channels', model_params['in_channels']),\n",
    "            out_classes=kwargs.get('out_classes', model_params['out_classes']),\n",
    "        )\n",
    "        \n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "def train_deep_model(deepvPlus:DeepV_plus):\n",
    "    wandb.finish()\n",
    "    # Initialize WandbLogger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"DeeplabV3Plus\",  # Your Wandb project name\n",
    "        name=f\"{deepvPlus.arch_name}-{deepvPlus.encoder_name}\",   # Experiment name\n",
    "        log_model=True,  # Log model checkpoints to Wandb\n",
    "        reinit=True \n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"deepCheckpoints/\",  # Directory to save checkpoints\n",
    "        filename=f\"{deepvPlus.arch_name}-{deepvPlus.encoder_name}\",  # Naming convention for the checkpoints\n",
    "        monitor=\"val_loss\",  # Metric to monitor for checkpoint saving\n",
    "        save_top_k=1,  # Save top 1 models with the best 'val_loss'\n",
    "        mode=\"min\",  # Save models with minimum 'val_loss'\n",
    "        save_last=True,  # Also save the latest checkpoint\n",
    "        verbose=True,  # Verbosity of saving messages\n",
    "        enable_version_counter=False,\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS, \n",
    "        log_every_n_steps=1, \n",
    "        callbacks=[checkpoint_callback],  # Add the checkpoint callback here\n",
    "        logger=wandb_logger  # Attach the Wandb logger\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        deepvPlus,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    valid_metrics = trainer.validate(deepvPlus, dataloaders=val_dataloader, verbose=False)\n",
    "    print(valid_metrics)\n",
    "\n",
    "    test_metrics = trainer.test(deepvPlus, dataloaders=test_dataloader, verbose=False)\n",
    "    print(test_metrics)\n",
    "\n",
    "    # smp_model = deepvPlus.model\n",
    "\n",
    "    # commit_info = smp_model.save_pretrained(\n",
    "    #     save_directory=\"saved_models/DeepLabv3Plus\",\n",
    "    # )\n",
    "\n",
    "    clear_gpu_memory(deepvPlus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_name):\n",
    "    # Load the model from the checkpoint\n",
    "    trainer = pl.Trainer(max_epochs=1000)\n",
    "    deepvPlus = DeepV_plus.load_from_checkpoint(f\"deepCheckpoints/{checkpoint_name}.ckpt\")\n",
    "    \n",
    "    valid_metrics = trainer.validate(deepvPlus, dataloaders=val_dataloader, verbose=False)\n",
    "    print(valid_metrics)\n",
    "\n",
    "    test_metrics = trainer.test(deepvPlus, dataloaders=test_dataloader, verbose=False)\n",
    "    print(test_metrics)\n",
    "    clear_gpu_memory(deepvPlus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2180153\u001b[0m (\u001b[33m2180153-wits-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240921_125024-p7zspisu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/p7zspisu' target=\"_blank\">deeplabv3plus-resnet34</a></strong> to <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/p7zspisu' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/p7zspisu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 22.4 M | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "22.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.751    Total estimated model params size (MB)\n",
      "173       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 20/20 [00:02<00:00, 10.00it/s, v_num=pisu, val_loss=0.346, valid_per_image_iou=0.781, valid_dataset_iou=0.780, valid_f1_score=0.877]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'val_loss' reached 0.31060 (best 0.31060), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 20/20 [00:01<00:00, 11.00it/s, v_num=pisu, val_loss=0.152, valid_per_image_iou=0.915, valid_dataset_iou=0.915, valid_f1_score=0.956, train_per_image_iou=0.754, train_dataset_iou=0.735, train_f1_score=0.847]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 40: 'val_loss' reached 0.16541 (best 0.16541), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 20/20 [00:01<00:00, 10.58it/s, v_num=pisu, val_loss=0.103, valid_per_image_iou=0.947, valid_dataset_iou=0.947, valid_f1_score=0.973, train_per_image_iou=0.872, train_dataset_iou=0.872, train_f1_score=0.932]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 60: 'val_loss' reached 0.12245 (best 0.12245), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 20/20 [00:01<00:00, 10.56it/s, v_num=pisu, val_loss=0.0845, valid_per_image_iou=0.963, valid_dataset_iou=0.963, valid_f1_score=0.981, train_per_image_iou=0.918, train_dataset_iou=0.918, train_f1_score=0.957]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 80: 'val_loss' reached 0.09736 (best 0.09736), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 20/20 [00:01<00:00, 10.28it/s, v_num=pisu, val_loss=0.0701, valid_per_image_iou=0.975, valid_dataset_iou=0.975, valid_f1_score=0.987, train_per_image_iou=0.940, train_dataset_iou=0.940, train_f1_score=0.969]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 100: 'val_loss' reached 0.07913 (best 0.07913), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 20/20 [00:01<00:00, 10.64it/s, v_num=pisu, val_loss=0.0625, valid_per_image_iou=0.968, valid_dataset_iou=0.968, valid_f1_score=0.984, train_per_image_iou=0.955, train_dataset_iou=0.955, train_f1_score=0.977]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 120: 'val_loss' reached 0.06596 (best 0.06596), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 20/20 [00:01<00:00, 10.61it/s, v_num=pisu, val_loss=0.0526, valid_per_image_iou=0.979, valid_dataset_iou=0.979, valid_f1_score=0.989, train_per_image_iou=0.966, train_dataset_iou=0.966, train_f1_score=0.983]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 140: 'val_loss' reached 0.05587 (best 0.05587), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|| 20/20 [00:01<00:00, 10.68it/s, v_num=pisu, val_loss=0.0453, valid_per_image_iou=0.982, valid_dataset_iou=0.982, valid_f1_score=0.991, train_per_image_iou=0.972, train_dataset_iou=0.972, train_f1_score=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 160: 'val_loss' reached 0.04787 (best 0.04787), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|| 20/20 [00:01<00:00, 10.40it/s, v_num=pisu, val_loss=0.0422, valid_per_image_iou=0.984, valid_dataset_iou=0.984, valid_f1_score=0.992, train_per_image_iou=0.977, train_dataset_iou=0.977, train_f1_score=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 180: 'val_loss' reached 0.04189 (best 0.04189), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 20/20 [00:01<00:00, 10.53it/s, v_num=pisu, val_loss=0.0369, valid_per_image_iou=0.984, valid_dataset_iou=0.984, valid_f1_score=0.992, train_per_image_iou=0.980, train_dataset_iou=0.980, train_f1_score=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 200: 'val_loss' reached 0.03687 (best 0.03687), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|| 20/20 [00:01<00:00, 10.51it/s, v_num=pisu, val_loss=0.0321, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.993, train_per_image_iou=0.983, train_dataset_iou=0.983, train_f1_score=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 220: 'val_loss' reached 0.03295 (best 0.03295), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|| 20/20 [00:01<00:00, 10.58it/s, v_num=pisu, val_loss=0.0295, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.984, train_dataset_iou=0.984, train_f1_score=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 240: 'val_loss' reached 0.02971 (best 0.02971), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 20/20 [00:01<00:00, 10.45it/s, v_num=pisu, val_loss=0.0284, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.986, train_dataset_iou=0.986, train_f1_score=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 260: 'val_loss' reached 0.02725 (best 0.02725), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|| 20/20 [00:01<00:00, 10.47it/s, v_num=pisu, val_loss=0.0246, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.987, train_dataset_iou=0.987, train_f1_score=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 280: 'val_loss' reached 0.02519 (best 0.02519), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|| 20/20 [00:01<00:00, 10.64it/s, v_num=pisu, val_loss=0.0243, valid_per_image_iou=0.989, valid_dataset_iou=0.989, valid_f1_score=0.994, train_per_image_iou=0.987, train_dataset_iou=0.987, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 300: 'val_loss' reached 0.02341 (best 0.02341), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|| 20/20 [00:01<00:00, 10.55it/s, v_num=pisu, val_loss=0.0226, valid_per_image_iou=0.989, valid_dataset_iou=0.989, valid_f1_score=0.994, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 320: 'val_loss' reached 0.02201 (best 0.02201), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|| 20/20 [00:01<00:00, 10.69it/s, v_num=pisu, val_loss=0.0219, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 340: 'val_loss' reached 0.02076 (best 0.02076), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|| 20/20 [00:01<00:00, 10.52it/s, v_num=pisu, val_loss=0.0205, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 360: 'val_loss' reached 0.01978 (best 0.01978), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|| 20/20 [00:01<00:00, 10.52it/s, v_num=pisu, val_loss=0.0196, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 380: 'val_loss' reached 0.01906 (best 0.01906), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 20/20 [00:01<00:00, 10.75it/s, v_num=pisu, val_loss=0.0191, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 400: 'val_loss' reached 0.01852 (best 0.01852), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|| 20/20 [00:01<00:00, 10.38it/s, v_num=pisu, val_loss=0.0182, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 420: 'val_loss' reached 0.01785 (best 0.01785), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|| 20/20 [00:01<00:00, 10.48it/s, v_num=pisu, val_loss=0.0181, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 440: 'val_loss' reached 0.01745 (best 0.01745), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|| 20/20 [00:01<00:00, 10.52it/s, v_num=pisu, val_loss=0.0175, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 460: 'val_loss' reached 0.01708 (best 0.01708), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|| 20/20 [00:01<00:00, 10.24it/s, v_num=pisu, val_loss=0.0168, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 480: 'val_loss' reached 0.01677 (best 0.01677), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|| 20/20 [00:01<00:00, 10.08it/s, v_num=pisu, val_loss=0.0172, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 500: 'val_loss' reached 0.01638 (best 0.01638), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|| 20/20 [00:01<00:00, 10.36it/s, v_num=pisu, val_loss=0.0168, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 520: 'val_loss' reached 0.01633 (best 0.01633), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|| 20/20 [00:01<00:00, 10.45it/s, v_num=pisu, val_loss=0.0171, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 540: 'val_loss' reached 0.01613 (best 0.01613), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|| 20/20 [00:01<00:00, 10.29it/s, v_num=pisu, val_loss=0.0162, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 560: 'val_loss' reached 0.01607 (best 0.01607), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|| 20/20 [00:01<00:00, 10.62it/s, v_num=pisu, val_loss=0.0163, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 580: 'val_loss' reached 0.01587 (best 0.01587), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 20/20 [00:01<00:00, 10.42it/s, v_num=pisu, val_loss=0.0162, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 600: 'val_loss' reached 0.01584 (best 0.01584), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 20/20 [00:05<00:00,  3.94it/s, v_num=pisu, val_loss=0.0162, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|| 2/2 [00:00<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.016173720359802246, 'valid_per_image_iou': 0.9909735321998596, 'valid_dataset_iou': 0.9909728169441223, 'valid_f1_score': 0.9954655170440674}]\n",
      "Testing DataLoader 0: 100%|| 4/4 [00:00<00:00, 44.01it/s]\n",
      "[{'val_loss': 0.01553952693939209, 'test_per_image_iou': 0.9910772442817688, 'test_dataset_iou': 0.9910755753517151, 'test_f1_score': 0.9955173134803772}]\n"
     ]
    }
   ],
   "source": [
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"resnet34\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_dataset_iou</td><td></td></tr><tr><td>test_f1_score</td><td></td></tr><tr><td>test_per_image_iou</td><td></td></tr><tr><td>train_dataset_iou</td><td></td></tr><tr><td>train_f1_score</td><td></td></tr><tr><td>train_per_image_iou</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>valid_dataset_iou</td><td></td></tr><tr><td>valid_f1_score</td><td></td></tr><tr><td>valid_per_image_iou</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>test_dataset_iou</td><td>0.99108</td></tr><tr><td>test_f1_score</td><td>0.99552</td></tr><tr><td>test_per_image_iou</td><td>0.99108</td></tr><tr><td>train_dataset_iou</td><td>0.99052</td></tr><tr><td>train_f1_score</td><td>0.99524</td></tr><tr><td>train_per_image_iou</td><td>0.99052</td></tr><tr><td>trainer/global_step</td><td>600</td></tr><tr><td>val_loss</td><td>0.01554</td></tr><tr><td>valid_dataset_iou</td><td>0.99097</td></tr><tr><td>valid_f1_score</td><td>0.99547</td></tr><tr><td>valid_per_image_iou</td><td>0.99097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deeplabv3plus-resnet34</strong> at: <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/p7zspisu' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/p7zspisu</a><br/> View project at: <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240921_125024-p7zspisu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240921_125320-vfdzhdac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/vfdzhdac' target=\"_blank\">deeplabv3plus-resnext50_32x4d</a></strong> to <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/vfdzhdac' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/vfdzhdac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 26.1 M | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.599   Total estimated model params size (MB)\n",
      "208       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 20/20 [00:03<00:00,  6.25it/s, v_num=hdac, val_loss=0.342, valid_per_image_iou=0.805, valid_dataset_iou=0.805, valid_f1_score=0.892]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'val_loss' reached 0.29444 (best 0.29444), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 20/20 [00:03<00:00,  6.04it/s, v_num=hdac, val_loss=0.163, valid_per_image_iou=0.876, valid_dataset_iou=0.876, valid_f1_score=0.934, train_per_image_iou=0.771, train_dataset_iou=0.761, train_f1_score=0.865]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 40: 'val_loss' reached 0.16729 (best 0.16729), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 20/20 [00:03<00:00,  5.97it/s, v_num=hdac, val_loss=0.106, valid_per_image_iou=0.957, valid_dataset_iou=0.957, valid_f1_score=0.978, train_per_image_iou=0.853, train_dataset_iou=0.853, train_f1_score=0.920]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 60: 'val_loss' reached 0.12167 (best 0.12167), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 20/20 [00:03<00:00,  5.98it/s, v_num=hdac, val_loss=0.0836, valid_per_image_iou=0.966, valid_dataset_iou=0.966, valid_f1_score=0.983, train_per_image_iou=0.913, train_dataset_iou=0.912, train_f1_score=0.954]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 80: 'val_loss' reached 0.09370 (best 0.09370), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 20/20 [00:03<00:00,  6.00it/s, v_num=hdac, val_loss=0.0638, valid_per_image_iou=0.974, valid_dataset_iou=0.974, valid_f1_score=0.987, train_per_image_iou=0.945, train_dataset_iou=0.945, train_f1_score=0.972]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 100: 'val_loss' reached 0.07444 (best 0.07444), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 20/20 [00:03<00:00,  5.80it/s, v_num=hdac, val_loss=0.0565, valid_per_image_iou=0.978, valid_dataset_iou=0.978, valid_f1_score=0.989, train_per_image_iou=0.963, train_dataset_iou=0.963, train_f1_score=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 120: 'val_loss' reached 0.06117 (best 0.06117), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 20/20 [00:03<00:00,  5.88it/s, v_num=hdac, val_loss=0.0474, valid_per_image_iou=0.983, valid_dataset_iou=0.983, valid_f1_score=0.991, train_per_image_iou=0.970, train_dataset_iou=0.970, train_f1_score=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 140: 'val_loss' reached 0.05071 (best 0.05071), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|| 20/20 [00:03<00:00,  6.00it/s, v_num=hdac, val_loss=0.0397, valid_per_image_iou=0.985, valid_dataset_iou=0.985, valid_f1_score=0.993, train_per_image_iou=0.977, train_dataset_iou=0.977, train_f1_score=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 160: 'val_loss' reached 0.04263 (best 0.04263), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|| 20/20 [00:03<00:00,  5.91it/s, v_num=hdac, val_loss=0.0346, valid_per_image_iou=0.986, valid_dataset_iou=0.986, valid_f1_score=0.993, train_per_image_iou=0.982, train_dataset_iou=0.982, train_f1_score=0.991]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 180: 'val_loss' reached 0.03667 (best 0.03667), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 20/20 [00:03<00:00,  5.92it/s, v_num=hdac, val_loss=0.0312, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.984, train_dataset_iou=0.984, train_f1_score=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 200: 'val_loss' reached 0.03228 (best 0.03228), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|| 20/20 [00:03<00:00,  5.94it/s, v_num=hdac, val_loss=0.0278, valid_per_image_iou=0.989, valid_dataset_iou=0.989, valid_f1_score=0.994, train_per_image_iou=0.986, train_dataset_iou=0.986, train_f1_score=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 220: 'val_loss' reached 0.02870 (best 0.02870), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|| 20/20 [00:03<00:00,  5.89it/s, v_num=hdac, val_loss=0.0252, valid_per_image_iou=0.989, valid_dataset_iou=0.989, valid_f1_score=0.995, train_per_image_iou=0.987, train_dataset_iou=0.987, train_f1_score=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 240: 'val_loss' reached 0.02603 (best 0.02603), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 20/20 [00:03<00:00,  5.92it/s, v_num=hdac, val_loss=0.0235, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 260: 'val_loss' reached 0.02390 (best 0.02390), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|| 20/20 [00:03<00:00,  5.92it/s, v_num=hdac, val_loss=0.0203, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 280: 'val_loss' reached 0.02212 (best 0.02212), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|| 20/20 [00:03<00:00,  5.84it/s, v_num=hdac, val_loss=0.0201, valid_per_image_iou=0.990, valid_dataset_iou=0.990, valid_f1_score=0.995, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 300: 'val_loss' reached 0.02053 (best 0.02053), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|| 20/20 [00:03<00:00,  5.79it/s, v_num=hdac, val_loss=0.0188, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.995, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 320: 'val_loss' reached 0.01934 (best 0.01934), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|| 20/20 [00:03<00:00,  5.77it/s, v_num=hdac, val_loss=0.0184, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.996, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 340: 'val_loss' reached 0.01840 (best 0.01840), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|| 20/20 [00:03<00:00,  5.91it/s, v_num=hdac, val_loss=0.0176, valid_per_image_iou=0.991, valid_dataset_iou=0.991, valid_f1_score=0.996, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 360: 'val_loss' reached 0.01759 (best 0.01759), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|| 20/20 [00:03<00:00,  5.91it/s, v_num=hdac, val_loss=0.0163, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.990, train_dataset_iou=0.990, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 380: 'val_loss' reached 0.01685 (best 0.01685), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 20/20 [00:03<00:00,  5.90it/s, v_num=hdac, val_loss=0.0162, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 400: 'val_loss' reached 0.01636 (best 0.01636), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|| 20/20 [00:03<00:00,  6.00it/s, v_num=hdac, val_loss=0.0157, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 420: 'val_loss' reached 0.01581 (best 0.01581), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|| 20/20 [00:03<00:00,  5.91it/s, v_num=hdac, val_loss=0.016, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 440: 'val_loss' reached 0.01552 (best 0.01552), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|| 20/20 [00:03<00:00,  5.80it/s, v_num=hdac, val_loss=0.0152, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 460: 'val_loss' reached 0.01516 (best 0.01516), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|| 20/20 [00:03<00:00,  5.99it/s, v_num=hdac, val_loss=0.0151, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 480: 'val_loss' reached 0.01487 (best 0.01487), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|| 20/20 [00:03<00:00,  5.87it/s, v_num=hdac, val_loss=0.015, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 500: 'val_loss' reached 0.01472 (best 0.01472), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|| 20/20 [00:03<00:00,  5.75it/s, v_num=hdac, val_loss=0.0149, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 520: 'val_loss' reached 0.01449 (best 0.01449), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|| 20/20 [00:03<00:00,  5.92it/s, v_num=hdac, val_loss=0.0144, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 540: 'val_loss' reached 0.01449 (best 0.01449), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|| 20/20 [00:03<00:00,  5.98it/s, v_num=hdac, val_loss=0.0142, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 560: 'val_loss' reached 0.01428 (best 0.01428), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|| 20/20 [00:03<00:00,  5.97it/s, v_num=hdac, val_loss=0.0144, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 580: 'val_loss' reached 0.01420 (best 0.01420), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 20/20 [00:03<00:00,  5.93it/s, v_num=hdac, val_loss=0.0145, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 600: 'val_loss' reached 0.01411 (best 0.01411), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 20/20 [00:06<00:00,  3.31it/s, v_num=hdac, val_loss=0.0145, valid_per_image_iou=0.992, valid_dataset_iou=0.992, valid_f1_score=0.996, train_per_image_iou=0.991, train_dataset_iou=0.991, train_f1_score=0.996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|| 2/2 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.014513224363327026, 'valid_per_image_iou': 0.9921114444732666, 'valid_dataset_iou': 0.9921113848686218, 'valid_f1_score': 0.996039628982544}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 4/4 [00:00<00:00, 31.21it/s]\n",
      "[{'val_loss': 0.013683512806892395, 'test_per_image_iou': 0.9918500185012817, 'test_dataset_iou': 0.9918493032455444, 'test_f1_score': 0.9959075450897217}]\n"
     ]
    }
   ],
   "source": [
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"resnext50_32x4d\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_dataset_iou</td><td></td></tr><tr><td>test_f1_score</td><td></td></tr><tr><td>test_per_image_iou</td><td></td></tr><tr><td>train_dataset_iou</td><td></td></tr><tr><td>train_f1_score</td><td></td></tr><tr><td>train_per_image_iou</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>valid_dataset_iou</td><td></td></tr><tr><td>valid_f1_score</td><td></td></tr><tr><td>valid_per_image_iou</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>test_dataset_iou</td><td>0.99185</td></tr><tr><td>test_f1_score</td><td>0.99591</td></tr><tr><td>test_per_image_iou</td><td>0.99185</td></tr><tr><td>train_dataset_iou</td><td>0.99138</td></tr><tr><td>train_f1_score</td><td>0.99567</td></tr><tr><td>train_per_image_iou</td><td>0.99138</td></tr><tr><td>trainer/global_step</td><td>600</td></tr><tr><td>val_loss</td><td>0.01368</td></tr><tr><td>valid_dataset_iou</td><td>0.99211</td></tr><tr><td>valid_f1_score</td><td>0.99604</td></tr><tr><td>valid_per_image_iou</td><td>0.99211</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deeplabv3plus-resnext50_32x4d</strong> at: <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/vfdzhdac' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/vfdzhdac</a><br/> View project at: <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240921_125320-vfdzhdac\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240921_125706-23fxqho7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032/runs/23fxqho7' target=\"_blank\">deeplabv3plus-timm-regnetx_032</a></strong> to <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032/runs/23fxqho7' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032/runs/23fxqho7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 16.1 M | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "16.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.1 M    Total params\n",
      "64.362    Total estimated model params size (MB)\n",
      "586       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 20/20 [00:04<00:00,  4.86it/s, v_num=qho7, val_loss=0.366, valid_per_image_iou=0.746, valid_dataset_iou=0.746, valid_f1_score=0.854]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'val_loss' reached 0.31195 (best 0.31195), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 20/20 [00:04<00:00,  4.89it/s, v_num=qho7, val_loss=0.168, valid_per_image_iou=0.870, valid_dataset_iou=0.870, valid_f1_score=0.930, train_per_image_iou=0.743, train_dataset_iou=0.729, train_f1_score=0.843]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 40: 'val_loss' reached 0.16316 (best 0.16316), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 20/20 [00:04<00:00,  4.87it/s, v_num=qho7, val_loss=0.112, valid_per_image_iou=0.923, valid_dataset_iou=0.923, valid_f1_score=0.960, train_per_image_iou=0.861, train_dataset_iou=0.861, train_f1_score=0.925]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 60: 'val_loss' reached 0.12036 (best 0.12036), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 20/20 [00:03<00:00,  5.03it/s, v_num=qho7, val_loss=0.0857, valid_per_image_iou=0.955, valid_dataset_iou=0.955, valid_f1_score=0.977, train_per_image_iou=0.905, train_dataset_iou=0.905, train_f1_score=0.950]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 80: 'val_loss' reached 0.09387 (best 0.09387), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 20/20 [00:03<00:00,  5.02it/s, v_num=qho7, val_loss=0.0677, valid_per_image_iou=0.972, valid_dataset_iou=0.972, valid_f1_score=0.986, train_per_image_iou=0.933, train_dataset_iou=0.933, train_f1_score=0.966]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 100: 'val_loss' reached 0.07396 (best 0.07396), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 20/20 [00:04<00:00,  4.83it/s, v_num=qho7, val_loss=0.0544, valid_per_image_iou=0.976, valid_dataset_iou=0.976, valid_f1_score=0.988, train_per_image_iou=0.954, train_dataset_iou=0.954, train_f1_score=0.977]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 120: 'val_loss' reached 0.05972 (best 0.05972), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 20/20 [00:04<00:00,  4.82it/s, v_num=qho7, val_loss=0.048, valid_per_image_iou=0.978, valid_dataset_iou=0.978, valid_f1_score=0.989, train_per_image_iou=0.966, train_dataset_iou=0.966, train_f1_score=0.982] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 140: 'val_loss' reached 0.04962 (best 0.04962), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|| 20/20 [00:04<00:00,  4.78it/s, v_num=qho7, val_loss=0.0405, valid_per_image_iou=0.981, valid_dataset_iou=0.981, valid_f1_score=0.990, train_per_image_iou=0.974, train_dataset_iou=0.974, train_f1_score=0.987]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 160: 'val_loss' reached 0.04230 (best 0.04230), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|| 20/20 [00:04<00:00,  4.65it/s, v_num=qho7, val_loss=0.0351, valid_per_image_iou=0.983, valid_dataset_iou=0.983, valid_f1_score=0.991, train_per_image_iou=0.977, train_dataset_iou=0.977, train_f1_score=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 180: 'val_loss' reached 0.03693 (best 0.03693), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 20/20 [00:04<00:00,  4.95it/s, v_num=qho7, val_loss=0.0325, valid_per_image_iou=0.983, valid_dataset_iou=0.983, valid_f1_score=0.991, train_per_image_iou=0.980, train_dataset_iou=0.980, train_f1_score=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 200: 'val_loss' reached 0.03274 (best 0.03274), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|| 20/20 [00:04<00:00,  4.64it/s, v_num=qho7, val_loss=0.0296, valid_per_image_iou=0.985, valid_dataset_iou=0.985, valid_f1_score=0.992, train_per_image_iou=0.981, train_dataset_iou=0.981, train_f1_score=0.991]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 220: 'val_loss' reached 0.02951 (best 0.02951), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|| 20/20 [00:04<00:00,  4.79it/s, v_num=qho7, val_loss=0.0266, valid_per_image_iou=0.985, valid_dataset_iou=0.985, valid_f1_score=0.993, train_per_image_iou=0.983, train_dataset_iou=0.983, train_f1_score=0.991]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 240: 'val_loss' reached 0.02687 (best 0.02687), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 20/20 [00:04<00:00,  4.69it/s, v_num=qho7, val_loss=0.0246, valid_per_image_iou=0.986, valid_dataset_iou=0.986, valid_f1_score=0.993, train_per_image_iou=0.984, train_dataset_iou=0.984, train_f1_score=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 260: 'val_loss' reached 0.02480 (best 0.02480), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|| 20/20 [00:04<00:00,  4.86it/s, v_num=qho7, val_loss=0.0236, valid_per_image_iou=0.985, valid_dataset_iou=0.985, valid_f1_score=0.992, train_per_image_iou=0.985, train_dataset_iou=0.985, train_f1_score=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 280: 'val_loss' reached 0.02299 (best 0.02299), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|| 20/20 [00:04<00:00,  4.97it/s, v_num=qho7, val_loss=0.022, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.993, train_per_image_iou=0.986, train_dataset_iou=0.986, train_f1_score=0.993] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 300: 'val_loss' reached 0.02161 (best 0.02161), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|| 20/20 [00:04<00:00,  4.99it/s, v_num=qho7, val_loss=0.0209, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.993, train_per_image_iou=0.986, train_dataset_iou=0.986, train_f1_score=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 320: 'val_loss' reached 0.02031 (best 0.02031), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|| 20/20 [00:04<00:00,  4.99it/s, v_num=qho7, val_loss=0.0199, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.993, train_per_image_iou=0.987, train_dataset_iou=0.987, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 340: 'val_loss' reached 0.01927 (best 0.01927), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|| 20/20 [00:04<00:00,  4.98it/s, v_num=qho7, val_loss=0.0193, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.994, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 360: 'val_loss' reached 0.01852 (best 0.01852), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|| 20/20 [00:04<00:00,  4.86it/s, v_num=qho7, val_loss=0.0188, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.994, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 380: 'val_loss' reached 0.01782 (best 0.01782), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 20/20 [00:04<00:00,  4.97it/s, v_num=qho7, val_loss=0.0184, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.994, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 400: 'val_loss' reached 0.01718 (best 0.01718), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|| 20/20 [00:04<00:00,  4.92it/s, v_num=qho7, val_loss=0.0185, valid_per_image_iou=0.987, valid_dataset_iou=0.987, valid_f1_score=0.993, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 420: 'val_loss' reached 0.01683 (best 0.01683), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|| 20/20 [00:04<00:00,  4.90it/s, v_num=qho7, val_loss=0.0175, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.988, train_dataset_iou=0.988, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 440: 'val_loss' reached 0.01625 (best 0.01625), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|| 20/20 [00:04<00:00,  4.78it/s, v_num=qho7, val_loss=0.0171, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 460: 'val_loss' reached 0.01602 (best 0.01602), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|| 20/20 [00:04<00:00,  4.97it/s, v_num=qho7, val_loss=0.0169, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 480: 'val_loss' reached 0.01571 (best 0.01571), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|| 20/20 [00:04<00:00,  4.89it/s, v_num=qho7, val_loss=0.017, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 500: 'val_loss' reached 0.01548 (best 0.01548), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|| 20/20 [00:04<00:00,  4.94it/s, v_num=qho7, val_loss=0.0167, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 520: 'val_loss' reached 0.01537 (best 0.01537), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|| 20/20 [00:04<00:00,  4.91it/s, v_num=qho7, val_loss=0.0167, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 540: 'val_loss' reached 0.01526 (best 0.01526), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|| 20/20 [00:04<00:00,  4.94it/s, v_num=qho7, val_loss=0.0164, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 560: 'val_loss' reached 0.01508 (best 0.01508), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|| 20/20 [00:04<00:00,  4.89it/s, v_num=qho7, val_loss=0.016, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.995] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 580: 'val_loss' reached 0.01503 (best 0.01503), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 20/20 [00:04<00:00,  4.81it/s, v_num=qho7, val_loss=0.0166, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 600: 'val_loss' reached 0.01489 (best 0.01489), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 20/20 [00:06<00:00,  3.27it/s, v_num=qho7, val_loss=0.0166, valid_per_image_iou=0.988, valid_dataset_iou=0.988, valid_f1_score=0.994, train_per_image_iou=0.989, train_dataset_iou=0.989, train_f1_score=0.995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|| 2/2 [00:00<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.016593188047409058, 'valid_per_image_iou': 0.9882156252861023, 'valid_dataset_iou': 0.9882150292396545, 'valid_f1_score': 0.9940721392631531}]\n",
      "Testing DataLoader 0: 100%|| 4/4 [00:00<00:00, 32.63it/s]\n",
      "[{'val_loss': 0.014644607901573181, 'test_per_image_iou': 0.9886478185653687, 'test_dataset_iou': 0.9886442422866821, 'test_f1_score': 0.994289219379425}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"timm-regnetx_032\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_38572\\3189538887.py:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n",
      "Validation DataLoader 0: 100%|| 2/2 [00:00<00:00, 37.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.016173720359802246, 'valid_per_image_iou': 0.9909735321998596, 'valid_dataset_iou': 0.9909728169441223, 'valid_f1_score': 0.9954655170440674}]\n",
      "Testing DataLoader 0: 100%|| 4/4 [00:00<00:00, 53.99it/s] \n",
      "[{'val_loss': 0.01553952693939209, 'test_per_image_iou': 0.9910772442817688, 'test_dataset_iou': 0.9910755753517151, 'test_f1_score': 0.9955173134803772}]\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(\"deeplabv3plus-resnet34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
