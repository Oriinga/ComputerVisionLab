{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import wandb\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html - documetnation on how to make a pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing \n",
    "- dataloaders\n",
    "- augmentation pipeline\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "# the images are loaded as float32 and normalised\n",
    "# the mask is thresholded at 0.5 \n",
    "\"\"\" the permute is needed since the format for image tensors must be (C, H, W)\n",
    "But when we read from opencv the shape is (H, W, C)\n",
    "and the mask must be of dim (1, H, W) since single channel - unsqueeze add this channel\n",
    "\"\"\"\n",
    "# returned as tensors \n",
    "\n",
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(img_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "        print(self.images)\n",
    "        print(self.masks)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # get the images\n",
    "        image = cv2.imread(img_path)\n",
    "        # cv2 gives BGR switch it to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #get masks\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #normalise the images and mask to float32\n",
    "        image = image/255.0\n",
    "        mask = mask / 255.0\n",
    "        #threshold the mask\n",
    "        \n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "        mask = torch.tensor(mask)\n",
    "        inverse_mask = 1- mask\n",
    "        combined_mask = torch.stack([inverse_mask,mask],dim=0)\n",
    "        # combined_mask = mask\n",
    "        # for any data augmentation we want to do since training with small set and to avoid overfitting\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(2,0,1), combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image-10.png', 'image-12.png', 'image-2.png', 'image-21.png', 'image-24.png', 'image-27.png', 'image-28.png', 'image-30.png', 'image-43.png', 'image-7.png']\n",
      "['mask-10.png', 'mask-12.png', 'mask-2.png', 'mask-21.png', 'mask-24.png', 'mask-27.png', 'mask-28.png', 'mask-30.png', 'mask-43.png', 'mask-7.png']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PuzzleDataset(img_dir = \"./images-1024x768/train/\",\n",
    "                            mask_dir = \"./masks-1024x768/train/\")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "train_loader = DataLoader(train_dataset,batch_size =1, shuffle=True)\n",
    "\n",
    "# # # to visualise the images + masks \n",
    "# image, mask = train_dataset[2]\n",
    "# image = (image.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "# print(mask.shape)\n",
    "# mask = mask.squeeze(0)\n",
    "# # # Display the images\n",
    "# plt.figure()\n",
    "# plt.subplot(1,2,1), plt.imshow(image)\n",
    "# plt.subplot(1,2,2), plt.imshow(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Unet Construction\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 1 : Using `torch.nn.ConvTranspose2d` for upsampling\n",
    "## Variant 1 : We removed the softmax in the unet with convolve to get to the required number of output classes (ask richard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this is the triple convolution, chat gpt says we should use normalization dont know if we should keep it\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.triple_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.triple_conv(x)\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv, self).__init__()\n",
    "        self.triple_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.triple_conv(x)\n",
    "\n",
    "    \n",
    "# the down module is what the unet uses during the first half \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "            DoubleConv(in_channels,out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret_ = self.conv_pool(x)\n",
    "        return ret_\n",
    "\n",
    "# up transpose, \n",
    "class UpConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConvTranspose, self).__init__()\n",
    "        # to determine amount of out channels\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = Conv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        # adding from the down section\n",
    "        #x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpBilinear(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpBilinear, self).__init__()\n",
    "        # the bilinear is provided by the nn module, we set the mode to bilinear here\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = Conv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return self.conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,mode):\n",
    "        super(Up, self).__init__()\n",
    "        if(mode=='convtranspose'):\n",
    "            self.conv_pool = nn.Sequential(\n",
    "                UpConvTranspose(in_channels,out_channels),\n",
    "                Conv(out_channels,out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv_pool = nn.Sequential(\n",
    "                UpBilinear(in_channels,out_channels),\n",
    "                Conv(out_channels,out_channels)\n",
    "            )\n",
    "        \n",
    "        self.dconv = DoubleConv(out_channels*2,out_channels)\n",
    "    \n",
    "    def forward(self,x1,x2):\n",
    "        x = self.conv_pool(x1)\n",
    "        return self.dconv(torch.cat([x,x2],dim=1))\n",
    "\n",
    "    \n",
    "class SoftMax(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return torch.argmax(x,dim=1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, variant='convtranspose'):\n",
    "        super(Unet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.variant = variant\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64,128)\n",
    "        self.down2 = Down(128,256)\n",
    "        self.down3 = Down(256,512)\n",
    "        self.down4 = Down(512,1024)\n",
    "        \n",
    "        self.up1 = Up(1024,512,variant)\n",
    "        self.up2 = Up(512, 256,variant)\n",
    "        self.up3 = Up(256, 128,variant)\n",
    "        self.up4 = Up(128, 64,variant)\n",
    "\n",
    "        self.outc = nn.Conv2d(64,2, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I just pushed to the cpu when I try putting the model on the gpu I get weird errors in training \n",
    "that I used up all the GPU memory maybe you wont get this error then just comment out `device=\"cpu\"\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n",
      "identifier torch.Size([1, 2, 768, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [07:20<00:00, 440.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "model = Unet(n_channels=3, n_classes=2, variant='convtranspose')  # Example: 3 input channels, 2 output classes\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(device)\n",
    "num_epochs = 1  # Set the number of epochs\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader):\n",
    "        # Move data to the device (GPU/CPU)\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        print(\"identifier\",masks.shape)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.float()\n",
    "        # print(\"outputs\",outputs.shape)\n",
    "        # print(\"masks\",masks.shape)\n",
    "        # Compute the loss\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# run tensorboard --logdir=runs to see network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image-10.png', 'image-12.png', 'image-2.png', 'image-21.png', 'image-24.png', 'image-27.png', 'image-28.png', 'image-30.png', 'image-43.png', 'image-7.png']\n",
      "['mask-10.png', 'mask-12.png', 'mask-2.png', 'mask-21.png', 'mask-24.png', 'mask-27.png', 'mask-28.png', 'mask-30.png', 'mask-43.png', 'mask-7.png']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = PuzzleDataset(img_dir = \"./images-1024x768/train/\",\n",
    "                            mask_dir = \"./masks-1024x768/train/\")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "test_loader = DataLoader(test_dataset,batch_size =1, shuffle=True)\n",
    "model.eval()\n",
    "for image,mask in test_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        #apply the softmax here not in the network\n",
    "        probs = torch.softmax(outputs, dim = 1 )\n",
    "\n",
    "        predicted_mask = torch.argmax(probs, dim=1)\n",
    "        plt.imshow(predicted_mask.squeeze(0).cpu().numpy(),cmap=\"gray\")\n",
    "        plt.show()\n",
    "        break\n",
    "    # just for the push\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 2: Using `torch.nn.Upsample` for bilinear upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Training and evaluation \n",
    "### Training\n",
    "- [ ] Implement the training loop with binary cross entropy loss for pixel-wise classification\n",
    "- [ ] Adam Optimiser to update the model parameters\n",
    "**Note** have regularly saved checkpoints and evaluate model in the validation set to avoid over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases integration\n",
    "Log the following:\n",
    "- [ ] Track BCE loss over epochs\n",
    "- [ ] IoU for both training and validation sets to monitor the segmentation performance\n",
    "\n",
    "### Need to submit from the wandb dashboard\n",
    "- Loss curves (training and validation over epochs)\n",
    "- IoU curves (training and validation over epochs)\n",
    "\n",
    "### Remember to comment on comment on whether the model is overfitting and how to recognise this and deal with the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "evaluate model on test set \n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- F1 score\n",
    "- IoU\n",
    "\n",
    "# Select the best model based on the validation IoU and report its performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Other architectures\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
