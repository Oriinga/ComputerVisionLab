{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage\n",
    "import imageio\n",
    "import mpmath\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html - documetnation on how to make a pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing \n",
    "- dataloaders\n",
    "- augmentation pipeline\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "# the images are loaded as float32 and normalised\n",
    "# the mask is thresholded at 0.5 \n",
    "\"\"\" the permute is needed since the format for image tensors must be (C, H, W)\n",
    "But when we read from opencv the shape is (H, W, C)\n",
    "and the mask must be of dim (1, H, W) since single channel - unsqueeze add this channel\n",
    "\"\"\"\n",
    "# returned as tensors \n",
    "\n",
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(img_dir)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "\n",
    "        # get the images\n",
    "        image = cv2.imread(img_path)\n",
    "        # cv2 gives BGR switch it to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #get masks\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        #normalise the images and mask to float32\n",
    "        image = image/255.0\n",
    "        mask = mask / 255.0\n",
    "        #threshold the mask\n",
    "        print(mask.dtype())\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        # for any data augmentation we want to do since training with small set and to avoid overfitting\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(2,0,1), torch.tensor(mask, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1653.452] global loadsave.cpp:241 findDecoder imread_('./masks-1024x768/train/image-12.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# to visualise the images + masks \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m image, mask \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m (image\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m, in \u001b[0;36mPuzzleDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#normalise the images and mask to float32\u001b[39;00m\n\u001b[1;32m     32\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m---> 33\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#threshold the mask\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mdtype())\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "train_dataset = PuzzleDataset(img_dir = \"./images-1024x768/train/\",\n",
    "                            mask_dir = \"./masks-1024x768/train/\")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "train_loader = DataLoader(train_dataset,batch_size =1, shuffle=True)\n",
    "\n",
    "# to visualise the images + masks \n",
    "image, mask = train_dataset[0]\n",
    "image = (image.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "# Display the images\n",
    "cv2.imshow('Image', image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Unet Construction\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 1 : Using `torch.nn.ConvTranspose2d` for upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 2: Using `torch.nn.Upsample` for bilinear upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Training and evaluation \n",
    "### Training\n",
    "- [ ] Implement the training loop with binary cross entropy loss for pixel-wise classification\n",
    "- [ ] Adam Optimiser to update the model parameters\n",
    "**Note** have regularly saved checkpoints and evaluate model in the validation set to avoid over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases integration\n",
    "Log the following:\n",
    "- [ ] Track BCE loss over epochs\n",
    "- [ ] IoU for both training and validation sets to monitor the segmentation performance\n",
    "\n",
    "### Need to submit from the wandb dashboard\n",
    "- Loss curves (training and validation over epochs)\n",
    "- IoU curves (training and validation over epochs)\n",
    "\n",
    "### Remember to comment on comment on whether the model is overfitting and how to recognise this and deal with the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "evaluate model on test set \n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- F1 score\n",
    "- IoU\n",
    "\n",
    "# Select the best model based on the validation IoU and report its performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Other architectures\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
