{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage\n",
    "import imageio\n",
    "import mpmath\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html - documetnation on how to make a pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing \n",
    "- dataloaders\n",
    "- augmentation pipeline\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "# the images are loaded as float32 and normalised\n",
    "# the mask is thresholded at 0.5 \n",
    "\"\"\" the permute is needed since the format for image tensors must be (C, H, W)\n",
    "But when we read from opencv the shape is (H, W, C)\n",
    "and the mask must be of dim (1, H, W) since single channel - unsqueeze add this channel\n",
    "\"\"\"\n",
    "# returned as tensors \n",
    "\n",
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(img_dir)\n",
    "        self.masks = os.listdir(mask_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # get the images\n",
    "        image = cv2.imread(img_path)\n",
    "        # cv2 gives BGR switch it to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #get masks\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        #normalise the images and mask to float32\n",
    "        image = image/255.0\n",
    "        mask = mask / 255.0\n",
    "        #threshold the mask\n",
    "        \n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "        mask = torch.tensor(mask)\n",
    "\n",
    "        # for any data augmentation we want to do since training with small set and to avoid overfitting\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(2,0,1), mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_dataset = PuzzleDataset(img_dir = \"./images-1024x768/train/\",\n",
    "                            mask_dir = \"./masks-1024x768/train/\")\n",
    "#since 10 images batches of 1 should be fine can do like batches of 2 i guess                        \n",
    "train_loader = DataLoader(train_dataset,batch_size =1, shuffle=True)\n",
    "\n",
    "# to visualise the images + masks \n",
    "image, mask = train_dataset[0]\n",
    "image = (image.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "# Display the images\n",
    "cv2.imshow('Image', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask12 = cv2.imread(\"/images-1024x768/train/image-2.png\")\n",
    "plt.imshow(\"Image\",test_mask12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Unet Construction\n",
    "## Add notes on this here (what is happening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 1 : Using `torch.nn.ConvTranspose2d` for upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairant 2: Using `torch.nn.Upsample` for bilinear upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Training and evaluation \n",
    "### Training\n",
    "- [ ] Implement the training loop with binary cross entropy loss for pixel-wise classification\n",
    "- [ ] Adam Optimiser to update the model parameters\n",
    "**Note** have regularly saved checkpoints and evaluate model in the validation set to avoid over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases integration\n",
    "Log the following:\n",
    "- [ ] Track BCE loss over epochs\n",
    "- [ ] IoU for both training and validation sets to monitor the segmentation performance\n",
    "\n",
    "### Need to submit from the wandb dashboard\n",
    "- Loss curves (training and validation over epochs)\n",
    "- IoU curves (training and validation over epochs)\n",
    "\n",
    "### Remember to comment on comment on whether the model is overfitting and how to recognise this and deal with the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "evaluate model on test set \n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- F1 score\n",
    "- IoU\n",
    "\n",
    "# Select the best model based on the validation IoU and report its performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Other architectures\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
