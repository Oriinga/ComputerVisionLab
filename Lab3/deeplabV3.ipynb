{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from skimage import transform as sktf\n",
    "from skimage.util import random_noise\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "import gc\n",
    "from torch.optim import lr_scheduler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory(model=None, data_loaders=None):\n",
    "    \n",
    "    if model is not None:\n",
    "        model.cpu()\n",
    "        del model\n",
    "    \n",
    "    \n",
    "    if data_loaders is not None:\n",
    "        for loader in data_loaders:\n",
    "            del loader  \n",
    "    \n",
    "    #  garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor_as_image(tensor, channel_num, channel_index, height_index, width_index):\n",
    "    # Move the tensor to CPU and convert it to a NumPy array\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    if channel_index == 1:\n",
    "        tensor_np = tensor_np.squeeze(0)\n",
    "\n",
    "        channel_index -=1\n",
    "        height_index-=1\n",
    "        width_index-=1\n",
    "        \n",
    "    # Handle single-channel (grayscale) image\n",
    "    if channel_num == 1:\n",
    "        image_np = tensor_np.squeeze(channel_index)  # Remove the channel dimension\n",
    "        plt.imshow(image_np, cmap=\"gray\")\n",
    "        plt.title(\"Single-channel image\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Handle two-channel image (display channels separately)\n",
    "    elif channel_num == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create 1 row, 2 columns\n",
    "        for i in range(2):\n",
    "            channel_image = tensor_np[i]  # Select each channel (e.g., 0 and 1)\n",
    "            axes[i].imshow(channel_image, cmap=\"gray\")\n",
    "            axes[i].set_title(f\"Channel {i}\")\n",
    "            # print(f\"Max value in channel {i}:\", np.max(channel_image))\n",
    "            # print(f\"Min value in channel {i}:\", np.min(channel_image))\n",
    "        plt.show()\n",
    "    \n",
    "    # Handle three-channel image (RGB)\n",
    "    elif channel_num == 3:\n",
    "        print(tensor_np.shape)\n",
    "        # Transpose from (channels, height, width) to (height, width, channels)\n",
    "        image_np = np.transpose(tensor_np, (height_index, width_index, channel_index))\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(\"Three-channel image (RGB)\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, num_transforms=0,include_inverse_mask=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.num_transforms = num_transforms\n",
    "        self.include_inverse_mask=include_inverse_mask\n",
    "        images = sorted(os.listdir(img_dir))\n",
    "        masks = sorted(os.listdir(mask_dir))\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            img_path = os.path.join(self.img_dir, images[i])\n",
    "            mask_path = os.path.join(self.mask_dir, masks[i])\n",
    "\n",
    "          \n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "            image = image.astype(np.float32)/255.0\n",
    "            # image = Image.fromarray(image)  \n",
    "\n",
    "            \n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (512,512))\n",
    "            mask = (mask > 0.5).astype(np.float32) \n",
    "            # mask = Image.fromarray(mask)  #   PIL image needed for transforms\n",
    "\n",
    "            # store the original image and mask\n",
    "            self.append_image_mask(image, mask)\n",
    "\n",
    "            # do transformations \n",
    "            for _ in range(self.num_transforms):\n",
    "                transformed_image, transformed_mask = self.apply_transform(image, mask)\n",
    "                self.append_image_mask(transformed_image, transformed_mask)\n",
    "\n",
    "    def apply_transform(self, image, mask):\n",
    "        \"\"\"Apply deterministic transformations to both image and mask\n",
    "        This is imortant since using the torchvision.transforms was givin a random transform\n",
    "        for both image and mask -> they didn't match up\"\"\"\n",
    "        if self.transform:\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                image = np.fliplr(image)\n",
    "                mask = np.fliplr(mask)\n",
    "\n",
    "           \n",
    "            if random.random() > 0.5:\n",
    "                image = np.flipud(image)\n",
    "                mask = np.flipud(mask)\n",
    "\n",
    "            # Apply rotation deterministically\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            image = sktf.rotate(image, angle, mode=\"edge\" , preserve_range=True)\n",
    "            mask = sktf.rotate(mask, angle, mode=\"edge\" , preserve_range=True)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def append_image_mask(self, image, mask):\n",
    "        \"\"\"need to store them as tensors.\"\"\"\n",
    "        image = torch.tensor(image.transpose((2, 0, 1)), dtype=torch.float32) # (C, H, W)\n",
    "        mask = torch.tensor(mask[None, ...], dtype=torch.float32)   # (1, H, W)\n",
    "\n",
    "       \n",
    "        if(self.include_inverse_mask):\n",
    "            inverse_mask = 1 - mask\n",
    "            combined_mask = torch.cat([inverse_mask, mask], dim=0)  # Combined (2, H, W)\n",
    "\n",
    "        \n",
    "            self.data.append((image, combined_mask))\n",
    "        else:\n",
    "            self.data.append((image,mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/train/\",\n",
    "    mask_dir=\"./masks-1024x768/train/\", \n",
    "    transform=True,\n",
    "    num_transforms=3  \n",
    ")\n",
    "\n",
    "val_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/val/\",\n",
    "    mask_dir=\"./masks-1024x768/val/\", \n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = PuzzleDataset(img_dir = \"./images-1024x768/test/\",\n",
    "                            mask_dir = \"./masks-1024x768/test/\")\n",
    "\n",
    "train_dataloader =DataLoader(train_dataset,batch_size =2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size =1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep lab v3\n",
    "EPOCHS = 10\n",
    "T_MAX = EPOCHS * len(train_dataloader)\n",
    "OUT_CLASSES = 2\n",
    "\n",
    "class DeepV_plus(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, pretrained=\"imagenet\", **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=pretrained,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.encoder_name = encoder_name\n",
    "        self.arch_name = arch\n",
    "        self.in_channels = in_channels\n",
    "        self.out_classes = out_classes\n",
    "        # preprocessing parameteres for image\n",
    "        print(pretrained)\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name,pretrained)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "        # initialize step metics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[0]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32,\n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[1]\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then\n",
    "        # apply thresholding\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask.long(), mask.long(), mode=\"binary\"\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        \n",
    "        losses = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(f\"val_loss\", losses, prog_bar=True)\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image\n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(\n",
    "            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n",
    "        )\n",
    "\n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset\n",
    "        # with \"empty\" images (images without target class) a large gap could be observed.\n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\")\n",
    "        # append the metics of each step to the\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        # empty set output list\n",
    "        self.training_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\")\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\")\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        # empty set output list\n",
    "        self.test_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "        return\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # Save hyperparameters in the checkpoint\n",
    "        checkpoint['hyper_parameters'] = {\n",
    "            'arch': self.arch_name,\n",
    "            'encoder_name': self.encoder_name,\n",
    "            'in_channels': self.in_channels,  # Assuming model has this attribute\n",
    "            'out_classes': self.out_classes,  # Assuming model has this attribute\n",
    "        }\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        \"\"\"\n",
    "        Load model weights from a checkpoint and instantiate the model.\n",
    "\n",
    "        Parameters:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        kwargs (dict): Additional arguments to pass to the model initialization.\n",
    "\n",
    "        Returns:\n",
    "        DeepV_plus: Model instance with weights loaded from the checkpoint.\n",
    "        \"\"\"\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "        \n",
    "        # Get model parameters from the checkpoint\n",
    "        model_params = checkpoint['hyper_parameters']\n",
    "\n",
    "        # Create a new instance of the model using parameters from the checkpoint\n",
    "        model = cls(\n",
    "            arch=kwargs.get('arch', model_params['arch']),\n",
    "            encoder_name=kwargs.get('encoder_name', model_params['encoder_name']),\n",
    "            in_channels=kwargs.get('in_channels', model_params['in_channels']),\n",
    "            out_classes=kwargs.get('out_classes', model_params['out_classes']),\n",
    "        )\n",
    "        \n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "def train_deep_model(deepvPlus:DeepV_plus):\n",
    "\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"deepCheckpoints/\",  # Directory to save checkpoints\n",
    "        filename=f\"{deepvPlus.arch_name}-{deepvPlus.encoder_name}\",  # Naming convention for the checkpoints\n",
    "        monitor=\"val_loss\",  # Metric to monitor for checkpoint saving\n",
    "        save_top_k=1,  # Save top 1 models with the best 'val_loss'\n",
    "        mode=\"min\",  # Save models with minimum 'val_loss'\n",
    "        save_last=True,  # Also save the latest checkpoint\n",
    "        verbose=True,  # Verbosity of saving messages\n",
    "        enable_version_counter=False,\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS, \n",
    "        log_every_n_steps=1, \n",
    "        callbacks=[checkpoint_callback],  # Add the checkpoint callback here\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        deepvPlus,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    valid_metrics = trainer.validate(deepvPlus, dataloaders=val_dataloader, verbose=False)\n",
    "    print(valid_metrics)\n",
    "\n",
    "    test_metrics = trainer.test(deepvPlus, dataloaders=test_dataloader, verbose=False)\n",
    "    print(test_metrics)\n",
    "\n",
    "    # smp_model = deepvPlus.model\n",
    "\n",
    "    # commit_info = smp_model.save_pretrained(\n",
    "    #     save_directory=\"saved_models/DeepLabv3Plus\",\n",
    "    # )\n",
    "\n",
    "    clear_gpu_memory(deepvPlus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_name):\n",
    "    # Load the model from the checkpoint\n",
    "    trainer = pl.Trainer(max_epochs=1000)\n",
    "    deepvPlus = DeepV_plus.load_from_checkpoint(f\"deepCheckpoints/{checkpoint_name}.ckpt\")\n",
    "    \n",
    "    valid_metrics = trainer.validate(deepvPlus, dataloaders=val_dataloader, verbose=False)\n",
    "    print(valid_metrics)\n",
    "\n",
    "    test_metrics = trainer.test(deepvPlus, dataloaders=test_dataloader, verbose=False)\n",
    "    print(test_metrics)\n",
    "    clear_gpu_memory(deepvPlus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 22.4 M | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "22.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.751    Total estimated model params size (MB)\n",
      "173       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 20/20 [00:01<00:00, 10.79it/s, v_num=47, val_loss=0.366, valid_per_image_iou=0.778, valid_dataset_iou=0.778]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'val_loss' reached 0.31550 (best 0.31550), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:01<00:00, 11.19it/s, v_num=47, val_loss=0.149, valid_per_image_iou=0.928, valid_dataset_iou=0.928, train_per_image_iou=0.747, train_dataset_iou=0.727]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 40: 'val_loss' reached 0.16775 (best 0.16775), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 20/20 [00:01<00:00, 10.75it/s, v_num=47, val_loss=0.104, valid_per_image_iou=0.956, valid_dataset_iou=0.956, train_per_image_iou=0.872, train_dataset_iou=0.871]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 60: 'val_loss' reached 0.12168 (best 0.12168), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 20/20 [00:01<00:00, 10.76it/s, v_num=47, val_loss=0.0853, valid_per_image_iou=0.966, valid_dataset_iou=0.966, train_per_image_iou=0.921, train_dataset_iou=0.921]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 80: 'val_loss' reached 0.09745 (best 0.09745), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 20/20 [00:01<00:00, 10.70it/s, v_num=47, val_loss=0.0703, valid_per_image_iou=0.975, valid_dataset_iou=0.975, train_per_image_iou=0.943, train_dataset_iou=0.943]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 100: 'val_loss' reached 0.08212 (best 0.08212), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 20/20 [00:01<00:00, 10.79it/s, v_num=47, val_loss=0.0637, valid_per_image_iou=0.977, valid_dataset_iou=0.977, train_per_image_iou=0.956, train_dataset_iou=0.956]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 120: 'val_loss' reached 0.07214 (best 0.07214), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 20/20 [00:01<00:00, 10.75it/s, v_num=47, val_loss=0.0598, valid_per_image_iou=0.979, valid_dataset_iou=0.979, train_per_image_iou=0.964, train_dataset_iou=0.964]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 140: 'val_loss' reached 0.06572 (best 0.06572), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 20/20 [00:01<00:00, 10.80it/s, v_num=47, val_loss=0.0578, valid_per_image_iou=0.980, valid_dataset_iou=0.980, train_per_image_iou=0.968, train_dataset_iou=0.968]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 160: 'val_loss' reached 0.06191 (best 0.06191), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 20/20 [00:01<00:00, 10.61it/s, v_num=47, val_loss=0.0567, valid_per_image_iou=0.980, valid_dataset_iou=0.980, train_per_image_iou=0.971, train_dataset_iou=0.971]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 180: 'val_loss' reached 0.05983 (best 0.05983), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:01<00:00, 10.53it/s, v_num=47, val_loss=0.0553, valid_per_image_iou=0.980, valid_dataset_iou=0.980, train_per_image_iou=0.973, train_dataset_iou=0.973]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 200: 'val_loss' reached 0.05913 (best 0.05913), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnet34.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:04<00:00,  4.83it/s, v_num=47, val_loss=0.0553, valid_per_image_iou=0.980, valid_dataset_iou=0.980, train_per_image_iou=0.973, train_dataset_iou=0.973]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 16.55it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.05528825521469116, 'valid_per_image_iou': 0.9802222847938538, 'valid_dataset_iou': 0.9802173972129822}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 4/4 [00:00<00:00, 40.53it/s]\n",
      "[{'val_loss': 0.05438663065433502, 'test_per_image_iou': 0.9776372909545898, 'test_dataset_iou': 0.9776180982589722}]\n"
     ]
    }
   ],
   "source": [
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"resnet34\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth\" to C:\\Users\\willi/.cache\\torch\\hub\\checkpoints\\ig_resnext101_32x48-3e41cc8a.pth\n",
      "100%|██████████| 3.09G/3.09G [04:29<00:00, 12.3MB/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instagram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 829 M  | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "829 M     Trainable params\n",
      "0         Non-trainable params\n",
      "829 M     Total params\n",
      "3,318.128 Total estimated model params size (MB)\n",
      "344       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/20 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"resnext50_32x4d\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"timm-regnety_320\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_39172\\443083922.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'val_loss': 0.05107814073562622, 'valid_per_image_iou': 0.9801949262619019, 'valid_dataset_iou': 0.980192244052887}]\n",
      "Testing DataLoader 0: 100%|██████████| 4/4 [00:00<00:00, 47.12it/s] \n",
      "[{'val_loss': 0.050496652722358704, 'test_per_image_iou': 0.9772604703903198, 'test_dataset_iou': 0.9772445559501648}]\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(\"deeplabv3plus-resnet34\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
