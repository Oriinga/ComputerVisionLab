{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from skimage import transform as sktf\n",
    "from skimage.util import random_noise\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "import gc\n",
    "from torch.optim import lr_scheduler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory(model=None, data_loaders=None):\n",
    "    \n",
    "    if model is not None:\n",
    "        model.cpu()\n",
    "        del model\n",
    "    \n",
    "    \n",
    "    if data_loaders is not None:\n",
    "        for loader in data_loaders:\n",
    "            del loader  \n",
    "    \n",
    "    #  garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor_as_image(tensor, channel_num, channel_index, height_index, width_index):\n",
    "    # Move the tensor to CPU and convert it to a NumPy array\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    if channel_index == 1:\n",
    "        tensor_np = tensor_np.squeeze(0)\n",
    "\n",
    "        channel_index -=1\n",
    "        height_index-=1\n",
    "        width_index-=1\n",
    "        \n",
    "    # Handle single-channel (grayscale) image\n",
    "    if channel_num == 1:\n",
    "        image_np = tensor_np.squeeze(channel_index)  # Remove the channel dimension\n",
    "        plt.imshow(image_np, cmap=\"gray\")\n",
    "        plt.title(\"Single-channel image\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Handle two-channel image (display channels separately)\n",
    "    elif channel_num == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create 1 row, 2 columns\n",
    "        for i in range(2):\n",
    "            channel_image = tensor_np[i]  # Select each channel (e.g., 0 and 1)\n",
    "            axes[i].imshow(channel_image, cmap=\"gray\")\n",
    "            axes[i].set_title(f\"Channel {i}\")\n",
    "            # print(f\"Max value in channel {i}:\", np.max(channel_image))\n",
    "            # print(f\"Min value in channel {i}:\", np.min(channel_image))\n",
    "        plt.show()\n",
    "    \n",
    "    # Handle three-channel image (RGB)\n",
    "    elif channel_num == 3:\n",
    "        print(tensor_np.shape)\n",
    "        # Transpose from (channels, height, width) to (height, width, channels)\n",
    "        image_np = np.transpose(tensor_np, (height_index, width_index, channel_index))\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(\"Three-channel image (RGB)\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, num_transforms=0,include_inverse_mask=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.num_transforms = num_transforms\n",
    "        self.include_inverse_mask=include_inverse_mask\n",
    "        images = sorted(os.listdir(img_dir))\n",
    "        masks = sorted(os.listdir(mask_dir))\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            img_path = os.path.join(self.img_dir, images[i])\n",
    "            mask_path = os.path.join(self.mask_dir, masks[i])\n",
    "\n",
    "          \n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "            image = image.astype(np.float32)/255.0\n",
    "            # image = Image.fromarray(image)  \n",
    "\n",
    "            \n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (512,512))\n",
    "            mask = (mask > 0.5).astype(np.float32) \n",
    "            # mask = Image.fromarray(mask)  #   PIL image needed for transforms\n",
    "\n",
    "            # store the original image and mask\n",
    "            self.append_image_mask(image, mask)\n",
    "\n",
    "            # do transformations \n",
    "            for _ in range(self.num_transforms):\n",
    "                transformed_image, transformed_mask = self.apply_transform(image, mask)\n",
    "                self.append_image_mask(transformed_image, transformed_mask)\n",
    "\n",
    "    def apply_transform(self, image, mask):\n",
    "        \"\"\"Apply deterministic transformations to both image and mask\n",
    "        This is imortant since using the torchvision.transforms was givin a random transform\n",
    "        for both image and mask -> they didn't match up\"\"\"\n",
    "        if self.transform:\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                image = np.fliplr(image)\n",
    "                mask = np.fliplr(mask)\n",
    "\n",
    "           \n",
    "            if random.random() > 0.5:\n",
    "                image = np.flipud(image)\n",
    "                mask = np.flipud(mask)\n",
    "\n",
    "            # Apply rotation deterministically\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            image = sktf.rotate(image, angle, mode=\"edge\" , preserve_range=True)\n",
    "            mask = sktf.rotate(mask, angle, mode=\"edge\" , preserve_range=True)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def append_image_mask(self, image, mask):\n",
    "        \"\"\"need to store them as tensors.\"\"\"\n",
    "        image = torch.tensor(image.transpose((2, 0, 1)), dtype=torch.float32) # (C, H, W)\n",
    "        mask = torch.tensor(mask[None, ...], dtype=torch.float32)   # (1, H, W)\n",
    "\n",
    "       \n",
    "        if(self.include_inverse_mask):\n",
    "            inverse_mask = 1 - mask\n",
    "            combined_mask = torch.cat([inverse_mask, mask], dim=0)  # Combined (2, H, W)\n",
    "\n",
    "        \n",
    "            self.data.append((image, combined_mask))\n",
    "        else:\n",
    "            self.data.append((image,mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/train/\",\n",
    "    mask_dir=\"./masks-1024x768/train/\", \n",
    "    transform=True,\n",
    "    num_transforms=3  \n",
    ")\n",
    "\n",
    "val_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/val/\",\n",
    "    mask_dir=\"./masks-1024x768/val/\", \n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = PuzzleDataset(img_dir = \"./images-1024x768/test/\",\n",
    "                            mask_dir = \"./masks-1024x768/test/\")\n",
    "\n",
    "train_dataloader =DataLoader(train_dataset,batch_size =2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size =1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep lab v3\n",
    "EPOCHS = 30\n",
    "T_MAX = EPOCHS * len(train_dataloader)\n",
    "OUT_CLASSES = 2\n",
    "\n",
    "class DeepV_plus(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, pretrained=\"imagenet\", **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=pretrained,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.encoder_name = encoder_name\n",
    "        self.arch_name = arch\n",
    "        self.in_channels = in_channels\n",
    "        self.out_classes = out_classes\n",
    "        # preprocessing parameteres for image\n",
    "        print(pretrained)\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name,pretrained)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "        # initialize step metics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[0]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32,\n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[1]\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then\n",
    "        # apply thresholding\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask.long(), mask.long(), mode=\"binary\"\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        \n",
    "        losses = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(f\"val_loss\", losses, prog_bar=True)\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        #F1 score\n",
    "        precision = tp.sum() / (tp.sum() + fp.sum() + 1e-6)  # Add small value to avoid division by zero\n",
    "        recall = tp.sum() / (tp.sum() + fn.sum() + 1e-6)     # Add small value to avoid division by zero\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)    \n",
    "        # per image IoU means that we first calculate IoU score for each image\n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(\n",
    "            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n",
    "        )\n",
    "\n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset\n",
    "        # with \"empty\" images (images without target class) a large gap could be observed.\n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "            f\"{stage}_f1_score\": f1_score  # Log the F1 score\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\")\n",
    "        # append the metics of each step to the\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        # empty set output list\n",
    "        self.training_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\")\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\")\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        # empty set output list\n",
    "        self.test_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "        return\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # Save hyperparameters in the checkpoint\n",
    "        checkpoint['hyper_parameters'] = {\n",
    "            'arch': self.arch_name,\n",
    "            'encoder_name': self.encoder_name,\n",
    "            'in_channels': self.in_channels,  # Assuming model has this attribute\n",
    "            'out_classes': self.out_classes,  # Assuming model has this attribute\n",
    "        }\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        \"\"\"\n",
    "        Load model weights from a checkpoint and instantiate the model.\n",
    "\n",
    "        Parameters:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        kwargs (dict): Additional arguments to pass to the model initialization.\n",
    "\n",
    "        Returns:\n",
    "        DeepV_plus: Model instance with weights loaded from the checkpoint.\n",
    "        \"\"\"\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "        \n",
    "        # Get model parameters from the checkpoint\n",
    "        model_params = checkpoint['hyper_parameters']\n",
    "\n",
    "        # Create a new instance of the model using parameters from the checkpoint\n",
    "        model = cls(\n",
    "            arch=kwargs.get('arch', model_params['arch']),\n",
    "            encoder_name=kwargs.get('encoder_name', model_params['encoder_name']),\n",
    "            in_channels=kwargs.get('in_channels', model_params['in_channels']),\n",
    "            out_classes=kwargs.get('out_classes', model_params['out_classes']),\n",
    "        )\n",
    "        \n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "def train_deep_model(deepvPlus:DeepV_plus):\n",
    "    wandb.finish()\n",
    "    # Initialize WandbLogger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=f\"{deepvPlus.arch_name}-{deepvPlus.encoder_name}\",  # Your Wandb project name\n",
    "        name=f\"{deepvPlus.arch_name}-{deepvPlus.encoder_name}\",   # Experiment name\n",
    "        log_model=True,  # Log model checkpoints to Wandb\n",
    "        reinit=True \n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"deepCheckpoints/\",  # Directory to save checkpoints\n",
    "        filename=f\"{deepvPlus.arch_name}-{deepvPlus.encoder_name}\",  # Naming convention for the checkpoints\n",
    "        monitor=\"val_loss\",  # Metric to monitor for checkpoint saving\n",
    "        save_top_k=1,  # Save top 1 models with the best 'val_loss'\n",
    "        mode=\"min\",  # Save models with minimum 'val_loss'\n",
    "        save_last=True,  # Also save the latest checkpoint\n",
    "        verbose=True,  # Verbosity of saving messages\n",
    "        enable_version_counter=False,\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS, \n",
    "        log_every_n_steps=1, \n",
    "        callbacks=[checkpoint_callback],  # Add the checkpoint callback here\n",
    "        logger=wandb_logger  # Attach the Wandb logger\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        deepvPlus,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    valid_metrics = trainer.validate(deepvPlus, dataloaders=val_dataloader, verbose=False)\n",
    "    print(valid_metrics)\n",
    "\n",
    "    test_metrics = trainer.test(deepvPlus, dataloaders=test_dataloader, verbose=False)\n",
    "    print(test_metrics)\n",
    "\n",
    "    # smp_model = deepvPlus.model\n",
    "\n",
    "    # commit_info = smp_model.save_pretrained(\n",
    "    #     save_directory=\"saved_models/DeepLabv3Plus\",\n",
    "    # )\n",
    "\n",
    "    clear_gpu_memory(deepvPlus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_name):\n",
    "    # Load the model from the checkpoint\n",
    "    trainer = pl.Trainer(max_epochs=1000)\n",
    "    deepvPlus = DeepV_plus.load_from_checkpoint(f\"deepCheckpoints/{checkpoint_name}.ckpt\")\n",
    "    \n",
    "    valid_metrics = trainer.validate(deepvPlus, dataloaders=val_dataloader, verbose=False)\n",
    "    print(valid_metrics)\n",
    "\n",
    "    test_metrics = trainer.test(deepvPlus, dataloaders=test_dataloader, verbose=False)\n",
    "    print(test_metrics)\n",
    "    clear_gpu_memory(deepvPlus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m deepvPlus_model \u001b[38;5;241m=\u001b[39m DeepV_plus(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeeplabv3plus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_deep_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepvPlus_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m, in \u001b[0;36mtrain_deep_model\u001b[1;34m(deepvPlus)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_deep_model\u001b[39m(deepvPlus:DeepV_plus):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize WandbLogger\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     wandb_logger \u001b[38;5;241m=\u001b[39m WandbLogger(\n\u001b[0;32m      8\u001b[0m         project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeepvPlus\u001b[38;5;241m.\u001b[39march_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeepvPlus\u001b[38;5;241m.\u001b[39mencoder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Your Wandb project name\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeepvPlus\u001b[38;5;241m.\u001b[39march_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeepvPlus\u001b[38;5;241m.\u001b[39mencoder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;66;03m# Experiment name\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         log_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Log model checkpoints to Wandb\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         reinit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[0;32m     12\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:4351\u001b[0m, in \u001b[0;36mfinish\u001b[1;34m(exit_code, quiet)\u001b[0m\n\u001b[0;32m   4341\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[0;32m   4342\u001b[0m \n\u001b[0;32m   4343\u001b[0m \u001b[38;5;124;03mThis is used when creating multiple runs in the same process.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4348\u001b[0m \u001b[38;5;124;03m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[0;32m   4349\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[1;32m-> 4351\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:452\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:393\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2153\u001b[0m, in \u001b[0;36mRun.finish\u001b[1;34m(self, exit_code, quiet)\u001b[0m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\n\u001b[0;32m   2142\u001b[0m     \u001b[38;5;28mself\u001b[39m, exit_code: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[0;32m   2145\u001b[0m \n\u001b[0;32m   2146\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2187\u001b[0m, in \u001b[0;36mRun._finish\u001b[1;34m(self, exit_code, quiet)\u001b[0m\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2189\u001b[0m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2436\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[1;34m(self, exit_code)\u001b[0m\n\u001b[0;32m   2433\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mresume_fname)\n\u001b[0;32m   2435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2436\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   2439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2681\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2677\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating current version\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2678\u001b[0m version_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_check_version(\n\u001b[0;32m   2679\u001b[0m     current_version\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m   2680\u001b[0m )\n\u001b[1;32m-> 2681\u001b[0m version_result \u001b[38;5;241m=\u001b[39m \u001b[43mversion_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version_result:\n\u001b[0;32m   2683\u001b[0m     version_handle\u001b[38;5;241m.\u001b[39mabandon()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"resnet34\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>test_dataset_iou</td><td>▁</td></tr><tr><td>test_per_image_iou</td><td>▁</td></tr><tr><td>train_dataset_iou</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_per_image_iou</td><td>▁▅▆▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇████</td></tr><tr><td>val_loss</td><td>█▇▄▄▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_dataset_iou</td><td>▁▃▇▇███████</td></tr><tr><td>valid_per_image_iou</td><td>▁▃▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_dataset_iou</td><td>0.9736</td></tr><tr><td>test_per_image_iou</td><td>0.9736</td></tr><tr><td>train_dataset_iou</td><td>0.96923</td></tr><tr><td>train_per_image_iou</td><td>0.96925</td></tr><tr><td>trainer/global_step</td><td>200</td></tr><tr><td>val_loss</td><td>0.06118</td></tr><tr><td>valid_dataset_iou</td><td>0.97854</td></tr><tr><td>valid_per_image_iou</td><td>0.97855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deeplabv3plus-resnet34</strong> at: <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/ge816zzl' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34/runs/ge816zzl</a><br/> View project at: <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnet34</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240921_122508-ge816zzl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240921_122628-sfy64g9y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/sfy64g9y' target=\"_blank\">deeplabv3plus-resnext50_32x4d</a></strong> to <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/sfy64g9y' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-resnext50_32x4d/runs/sfy64g9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 26.1 M | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.599   Total estimated model params size (MB)\n",
      "208       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 20/20 [00:03<00:00,  6.08it/s, v_num=4g9y, val_loss=0.370, valid_per_image_iou=0.813, valid_dataset_iou=0.813]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'val_loss' reached 0.30043 (best 0.30043), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:03<00:00,  5.91it/s, v_num=4g9y, val_loss=0.160, valid_per_image_iou=0.888, valid_dataset_iou=0.888, train_per_image_iou=0.741, train_dataset_iou=0.736]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 40: 'val_loss' reached 0.15920 (best 0.15920), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 20/20 [00:03<00:00,  5.90it/s, v_num=4g9y, val_loss=0.115, valid_per_image_iou=0.954, valid_dataset_iou=0.954, train_per_image_iou=0.853, train_dataset_iou=0.852]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 60: 'val_loss' reached 0.11580 (best 0.11580), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 20/20 [00:03<00:00,  5.90it/s, v_num=4g9y, val_loss=0.0864, valid_per_image_iou=0.962, valid_dataset_iou=0.962, train_per_image_iou=0.917, train_dataset_iou=0.916]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 80: 'val_loss' reached 0.09048 (best 0.09048), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 20/20 [00:03<00:00,  5.92it/s, v_num=4g9y, val_loss=0.069, valid_per_image_iou=0.971, valid_dataset_iou=0.971, train_per_image_iou=0.949, train_dataset_iou=0.948] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 100: 'val_loss' reached 0.07558 (best 0.07558), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 20/20 [00:03<00:00,  5.90it/s, v_num=4g9y, val_loss=0.0628, valid_per_image_iou=0.977, valid_dataset_iou=0.977, train_per_image_iou=0.963, train_dataset_iou=0.963]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 120: 'val_loss' reached 0.06631 (best 0.06631), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 20/20 [00:03<00:00,  5.81it/s, v_num=4g9y, val_loss=0.0587, valid_per_image_iou=0.978, valid_dataset_iou=0.978, train_per_image_iou=0.969, train_dataset_iou=0.969]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 140: 'val_loss' reached 0.06077 (best 0.06077), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 20/20 [00:03<00:00,  6.03it/s, v_num=4g9y, val_loss=0.0555, valid_per_image_iou=0.981, valid_dataset_iou=0.981, train_per_image_iou=0.973, train_dataset_iou=0.973]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 160: 'val_loss' reached 0.05721 (best 0.05721), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 20/20 [00:03<00:00,  5.80it/s, v_num=4g9y, val_loss=0.0542, valid_per_image_iou=0.980, valid_dataset_iou=0.980, train_per_image_iou=0.975, train_dataset_iou=0.975]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 180: 'val_loss' reached 0.05481 (best 0.05481), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:03<00:00,  5.84it/s, v_num=4g9y, val_loss=0.0535, valid_per_image_iou=0.982, valid_dataset_iou=0.982, train_per_image_iou=0.977, train_dataset_iou=0.977]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 200: 'val_loss' reached 0.05395 (best 0.05395), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-resnext50_32x4d.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:06<00:00,  3.11it/s, v_num=4g9y, val_loss=0.0535, valid_per_image_iou=0.982, valid_dataset_iou=0.982, train_per_image_iou=0.977, train_dataset_iou=0.977]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'val_loss': 0.05352580547332764, 'valid_per_image_iou': 0.9822277426719666, 'valid_dataset_iou': 0.9822238087654114}]\n",
      "Testing DataLoader 0: 100%|██████████| 4/4 [00:00<00:00, 28.20it/s]\n",
      "[{'val_loss': 0.0501801073551178, 'test_per_image_iou': 0.9809534549713135, 'test_dataset_iou': 0.9809476137161255}]\n"
     ]
    }
   ],
   "source": [
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"resnext50_32x4d\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_032-ed0c7f7e.pth\" to C:\\Users\\willi/.cache\\torch\\hub\\checkpoints\\regnetx_032-ed0c7f7e.pth\n",
      "100%|██████████| 58.7M/58.7M [00:06<00:00, 9.02MB/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2180153\u001b[0m (\u001b[33m2180153-wits-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240921_123452-4ijr1cfo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032/runs/4ijr1cfo' target=\"_blank\">deeplabv3plus-timm-regnetx_032</a></strong> to <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032/runs/4ijr1cfo' target=\"_blank\">https://wandb.ai/2180153-wits-university/deeplabv3plus-timm-regnetx_032/runs/4ijr1cfo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Tumi\\Other Subjects\\CV\\ComputerVisionLab\\Lab3\\deepCheckpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model   | DeepLabV3Plus | 16.1 M | train\n",
      "1 | loss_fn | DiceLoss      | 0      | train\n",
      "--------------------------------------------------\n",
      "16.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.1 M    Total params\n",
      "64.362    Total estimated model params size (MB)\n",
      "586       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 20/20 [00:04<00:00,  4.78it/s, v_num=1cfo, val_loss=0.352, valid_per_image_iou=0.776, valid_dataset_iou=0.776]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 20: 'val_loss' reached 0.30746 (best 0.30746), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:04<00:00,  4.85it/s, v_num=1cfo, val_loss=0.156, valid_per_image_iou=0.875, valid_dataset_iou=0.875, train_per_image_iou=0.773, train_dataset_iou=0.768]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 40: 'val_loss' reached 0.15953 (best 0.15953), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 20/20 [00:04<00:00,  4.79it/s, v_num=1cfo, val_loss=0.113, valid_per_image_iou=0.924, valid_dataset_iou=0.924, train_per_image_iou=0.864, train_dataset_iou=0.864]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 60: 'val_loss' reached 0.11947 (best 0.11947), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 20/20 [00:04<00:00,  4.76it/s, v_num=1cfo, val_loss=0.0918, valid_per_image_iou=0.947, valid_dataset_iou=0.947, train_per_image_iou=0.903, train_dataset_iou=0.902]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 80: 'val_loss' reached 0.09554 (best 0.09554), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 20/20 [00:04<00:00,  4.74it/s, v_num=1cfo, val_loss=0.0763, valid_per_image_iou=0.960, valid_dataset_iou=0.960, train_per_image_iou=0.930, train_dataset_iou=0.929]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 100: 'val_loss' reached 0.07923 (best 0.07923), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 20/20 [00:04<00:00,  4.80it/s, v_num=1cfo, val_loss=0.0681, valid_per_image_iou=0.965, valid_dataset_iou=0.965, train_per_image_iou=0.949, train_dataset_iou=0.949]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 120: 'val_loss' reached 0.06892 (best 0.06892), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 20/20 [00:04<00:00,  4.70it/s, v_num=1cfo, val_loss=0.0633, valid_per_image_iou=0.968, valid_dataset_iou=0.968, train_per_image_iou=0.960, train_dataset_iou=0.960]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 140: 'val_loss' reached 0.06272 (best 0.06272), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 20/20 [00:04<00:00,  4.73it/s, v_num=1cfo, val_loss=0.0611, valid_per_image_iou=0.971, valid_dataset_iou=0.971, train_per_image_iou=0.965, train_dataset_iou=0.965]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 160: 'val_loss' reached 0.05910 (best 0.05910), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 20/20 [00:04<00:00,  4.90it/s, v_num=1cfo, val_loss=0.0593, valid_per_image_iou=0.971, valid_dataset_iou=0.971, train_per_image_iou=0.968, train_dataset_iou=0.968]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 180: 'val_loss' reached 0.05699 (best 0.05699), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:04<00:00,  4.87it/s, v_num=1cfo, val_loss=0.0592, valid_per_image_iou=0.971, valid_dataset_iou=0.971, train_per_image_iou=0.970, train_dataset_iou=0.970]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 200: 'val_loss' reached 0.05579 (best 0.05579), saving model to 'C:\\\\Tumi\\\\Other Subjects\\\\CV\\\\ComputerVisionLab\\\\Lab3\\\\deepCheckpoints\\\\deeplabv3plus-timm-regnetx_032.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:05<00:00,  3.37it/s, v_num=1cfo, val_loss=0.0592, valid_per_image_iou=0.971, valid_dataset_iou=0.971, train_per_image_iou=0.970, train_dataset_iou=0.970]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.059243202209472656, 'valid_per_image_iou': 0.9709789752960205, 'valid_dataset_iou': 0.9709761142730713}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 4/4 [00:00<00:00, 28.63it/s]\n",
      "[{'val_loss': 0.05722916126251221, 'test_per_image_iou': 0.9691983461380005, 'test_dataset_iou': 0.9691727757453918}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deepvPlus_model = DeepV_plus(\"deeplabv3plus\", \"timm-regnetx_032\", in_channels=3, out_classes=2)\n",
    "train_deep_model(deepvPlus_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_34464\\2424274214.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'val_loss': 0.05528825521469116, 'valid_per_image_iou': 0.9802222847938538, 'valid_dataset_iou': 0.9802173972129822}]\n",
      "Testing DataLoader 0: 100%|██████████| 4/4 [00:00<00:00, 36.91it/s]\n",
      "[{'val_loss': 0.05438663065433502, 'test_per_image_iou': 0.9776372909545898, 'test_dataset_iou': 0.9776180982589722}]\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(\"deeplabv3plus-resnet34\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
