{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efb765-0fda-4510-9757-f34d8c9aa137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN IMPORTS \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from skimage import transform as sktf\n",
    "from skimage.util import random_noise\n",
    "import random\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html - documetnation on how to make a pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1771aa6-0e0f-461a-ba6d-2ac941a47c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa934b0-63e0-466b-9f7f-2ab068db6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "# the images are loaded as float32 and normalised\n",
    "# the mask is thresholded at 0.5 \n",
    "\"\"\" the permute is needed since the format for image tensors must be (C, H, W)\n",
    "But when we read from opencv the shape is (H, W, C)\n",
    "and the mask must be of dim (1, H, W) since single channel - unsqueeze add this channel\n",
    "\"\"\"\n",
    "# returned as tensors \n",
    "\n",
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, num_transforms=0,include_inverse_mask=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.num_transforms = num_transforms\n",
    "        self.include_inverse_mask=include_inverse_mask\n",
    "        images = sorted(os.listdir(img_dir))\n",
    "        masks = sorted(os.listdir(mask_dir))\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            img_path = os.path.join(self.img_dir, images[i])\n",
    "            mask_path = os.path.join(self.mask_dir, masks[i])\n",
    "\n",
    "          \n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "            image = image.astype(np.float32)/255.0\n",
    "            # image = Image.fromarray(image)  \n",
    "\n",
    "            \n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (512,512))\n",
    "            mask = (mask > 0.5).astype(np.float32) \n",
    "            # mask = Image.fromarray(mask)  #   PIL image needed for transforms\n",
    "\n",
    "            # store the original image and mask\n",
    "            self.append_image_mask(image, mask)\n",
    "\n",
    "            # do transformations \n",
    "            for _ in range(self.num_transforms):\n",
    "                transformed_image, transformed_mask = self.apply_transform(image, mask)\n",
    "                self.append_image_mask(transformed_image, transformed_mask)\n",
    "\n",
    "    def apply_transform(self, image, mask):\n",
    "        \"\"\"Apply deterministic transformations to both image and mask\n",
    "        This is imortant since using the torchvision.transforms was givin a random transform\n",
    "        for both image and mask -> they didn't match up\"\"\"\n",
    "        if self.transform:\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                image = np.fliplr(image)\n",
    "                mask = np.fliplr(mask)\n",
    "\n",
    "           \n",
    "            if random.random() > 0.5:\n",
    "                image = np.flipud(image)\n",
    "                mask = np.flipud(mask)\n",
    "\n",
    "            # Apply rotation deterministically\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            image = sktf.rotate(image, angle, mode=\"edge\" , preserve_range=True)\n",
    "            mask = sktf.rotate(mask, angle, mode=\"edge\" , preserve_range=True)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def append_image_mask(self, image, mask):\n",
    "        \"\"\"need to store them as tensors.\"\"\"\n",
    "        image = torch.tensor(image.transpose((2, 0, 1)), dtype=torch.float32) # (C, H, W)\n",
    "        mask = torch.tensor(mask[None, ...], dtype=torch.float32)   # (1, H, W)\n",
    "\n",
    "       \n",
    "        if(self.include_inverse_mask):\n",
    "            inverse_mask = 1 - mask\n",
    "            combined_mask = torch.cat([inverse_mask, mask], dim=0)  # Combined (2, H, W)\n",
    "\n",
    "        \n",
    "            self.data.append((image, combined_mask))\n",
    "        else:\n",
    "            self.data.append((image,mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c28be6-4945-4731-bad5-a9966665c751",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73ad52-f099-4c0a-acc0-b46b5b6e5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/train/\",\n",
    "    mask_dir=\"./masks-1024x768/train/\", \n",
    "    transform=True,\n",
    "    num_transforms=3,\n",
    "    include_inverse_mask=False \n",
    ")\n",
    "val_dataset = PuzzleDataset(\n",
    "    img_dir=\"./images-1024x768/val/\",\n",
    "    mask_dir=\"./masks-1024x768/val/\",\n",
    "    include_inverse_mask=False\n",
    ")\n",
    "\n",
    "test_dataset = PuzzleDataset(\n",
    "    img_dir = \"./images-1024x768/test/\",\n",
    "    mask_dir = \"./masks-1024x768/test/\",\n",
    "    include_inverse_mask=False\n",
    "\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=5,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=5,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34559628-bac3-4ff2-b413-211e0d25cf27",
   "metadata": {},
   "source": [
    "# base class definition for unet from the library - Unet+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b454e-f6e7-462d-9a43-b5c8008f02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "T_MAX = EPOCHS * len(train_dataloader)\n",
    "OUT_CLASSES = 1\n",
    "\n",
    "class UnetPlus(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # preprocessing parameteres for image\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "        # initialize step metics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        #print(\"batch:\",batch[0].shape)\n",
    "        image, mask = batch\n",
    "        #image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32,\n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        #mask = batch[\"mask\"]\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "        #print(f\"Logits mask shape: {logits_mask.shape}, Target mask shape: {mask.shape}\")\n",
    "\n",
    "\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then\n",
    "        # apply thresholding\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask.long(), mask.long(), mode=\"binary\"\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image\n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(\n",
    "            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n",
    "        )\n",
    "\n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset\n",
    "        # with \"empty\" images (images without target class) a large gap could be observed.\n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\")\n",
    "        # append the metics of each step to the\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        # empty set output list\n",
    "        self.training_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\")\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\")\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        # empty set output list\n",
    "        self.test_step_outputs.clear()\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168ed59-eba0-405c-a899-ccbd1a640d61",
   "metadata": {},
   "source": [
    "### 1. Resnet34 Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f839086-05b9-449a-8030-bfd7803fb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "UnetRes = UnetPlus(\"Unet\",\"resnet34\",3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94e99e-7b6b-4a68-a688-66f1d6d540bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(\n",
    "    UnetRes,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebfb7d-88fc-4df6-b2d9-645836ec00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run validation dataset\n",
    "valid_metrics = trainer.validate(UnetRes, dataloaders=val_dataloader, verbose=False)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c256f10-a351-4c0e-a0d0-daaea54b790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test dataset\n",
    "test_metrics = trainer.test(UnetRes, dataloaders=test_dataloader, verbose=False)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f56302-e9a0-4635-9221-c1e99291cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "smp_model = UnetRes.model\n",
    "\n",
    "commit_info = smp_model.save_pretrained(\n",
    "    save_directory=\"saved_models/UnetPlus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2dee1-5fe6-4c7d-9b1f-365012bfbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and compare with Ground truth\n",
    "batch = next(iter(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(batch[\"image\"])\n",
    "pr_masks = logits.sigmoid()\n",
    "for idx, (image, gt_mask, pr_mask) in enumerate(\n",
    "    zip(batch[\"image\"], batch[\"mask\"], pr_masks)\n",
    "):\n",
    "    if idx <= 4:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(gt_mask.numpy().squeeze())\n",
    "        plt.title(\"Ground truth\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pr_mask.numpy().squeeze())\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee91e4-d5c9-4d61-8cea-8ed4d6ef43ed",
   "metadata": {},
   "source": [
    "### 2. VGG backbone for Unet+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d9bc2-5164-40ae-a246-5e568fa6197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UnetVGG = UnetPlus(\"Unet\",\"vgg11\",3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85423db7-6cdf-4210-8d9d-e4411611c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(\n",
    "    UnetVGG,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf98cea-4c19-43a8-8197-925445d6f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run validation dataset\n",
    "valid_metrics = trainer.validate(UnetVGG, dataloaders=val_dataloader, verbose=False)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562a5fc-24ed-498f-ac26-1c35ddeeb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test dataset\n",
    "test_metrics = trainer.test(UnetVGG, dataloaders=test_dataloader, verbose=False)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cd386-1fae-45af-9eb5-79b6db807bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "smp_model = UnetVGG.model\n",
    "\n",
    "commit_info = smp_model.save_pretrained(\n",
    "    save_directory=\"saved_models/UnetPlus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593006e-b92c-45e8-99c4-88d40d5a5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and compare with Ground truth\n",
    "batch = next(iter(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(batch[\"image\"])\n",
    "pr_masks = logits.sigmoid()\n",
    "for idx, (image, gt_mask, pr_mask) in enumerate(\n",
    "    zip(batch[\"image\"], batch[\"mask\"], pr_masks)\n",
    "):\n",
    "    if idx <= 4:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(gt_mask.numpy().squeeze())\n",
    "        plt.title(\"Ground truth\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pr_mask.numpy().squeeze())\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9590a-40d1-44a8-ad99-bb390cc9a2f5",
   "metadata": {},
   "source": [
    "### 3.  EfficientNet backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72941d0-f069-437d-8ef6-c5eeb988799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UnetEff = UnetPlus(\"Unet\",\"efficientnet-b7\",3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4626e33-f34a-463d-b4ca-24081ac02769",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS,log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(\n",
    "    UnetEff,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c103e05-0dca-45b4-b754-0f8f2e3ab86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run validation dataset\n",
    "valid_metrics = trainer.validate(UnetEff, dataloaders=val_dataloader, verbose=False)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a11158-747b-4470-8a7c-3ca9ff18eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test dataset\n",
    "test_metrics = trainer.test(UnetVGG, dataloaders=test_dataloader, verbose=False)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386932b-e650-41e1-9dbc-e3518c56bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "smp_model = UnetEff.model\n",
    "\n",
    "commit_info = smp_model.save_pretrained(\n",
    "    save_directory=\"saved_models/UnetPlus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6266c-c376-43f8-9d5e-9008f49390b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and compare with Ground truth\n",
    "batch = next(iter(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(batch[\"image\"])\n",
    "pr_masks = logits.sigmoid()\n",
    "for idx, (image, gt_mask, pr_mask) in enumerate(\n",
    "    zip(batch[\"image\"], batch[\"mask\"], pr_masks)\n",
    "):\n",
    "    if idx <= 4:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(gt_mask.numpy().squeeze())\n",
    "        plt.title(\"Ground truth\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pr_mask.numpy().squeeze())\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
